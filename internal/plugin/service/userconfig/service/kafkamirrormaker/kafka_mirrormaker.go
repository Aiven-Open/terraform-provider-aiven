// Code generated by user config generator. DO NOT EDIT.

package kafkamirrormaker

import (
	"context"
	"encoding/json"

	setvalidator "github.com/hashicorp/terraform-plugin-framework-validators/setvalidator"
	attr "github.com/hashicorp/terraform-plugin-framework/attr"
	datasource "github.com/hashicorp/terraform-plugin-framework/datasource/schema"
	diag "github.com/hashicorp/terraform-plugin-framework/diag"
	resource "github.com/hashicorp/terraform-plugin-framework/resource/schema"
	int64default "github.com/hashicorp/terraform-plugin-framework/resource/schema/int64default"
	validator "github.com/hashicorp/terraform-plugin-framework/schema/validator"
	types "github.com/hashicorp/terraform-plugin-framework/types"

	schemautil "github.com/aiven/terraform-provider-aiven/internal/schemautil"
)

// NewResourceSchema returns resource schema
func NewResourceSchema() resource.SetNestedBlock {
	return resource.SetNestedBlock{
		Description: "KafkaMirrormaker user configurable settings",
		NestedObject: resource.NestedBlockObject{
			Attributes: map[string]resource.Attribute{
				"additional_backup_regions": resource.SetAttribute{
					Computed:    true,
					Description: "Additional Cloud Regions for Backup Replication.",
					ElementType: types.StringType,
					Optional:    true,
					Validators:  []validator.Set{setvalidator.SizeAtMost(1)},
				},
				"static_ips": resource.BoolAttribute{
					Computed:    true,
					Description: "Use static public IP addresses.",
					Optional:    true,
				},
			},
			Blocks: map[string]resource.Block{
				"ip_filter": resource.SetNestedBlock{
					Description: "Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"description": resource.StringAttribute{
							Computed:    true,
							Description: "Description for IP filter list entry.",
							Optional:    true,
						},
						"network": resource.StringAttribute{
							Description: "CIDR address block.",
							Required:    true,
						},
					}},
					Validators: []validator.Set{setvalidator.SizeAtMost(1024)},
				},
				"kafka_mirrormaker": resource.SetNestedBlock{
					Description: "Kafka MirrorMaker configuration values",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"emit_checkpoints_enabled": resource.BoolAttribute{
							Computed:    true,
							Description: "Whether to emit consumer group offset checkpoints to target cluster periodically (default: true).",
							Optional:    true,
						},
						"emit_checkpoints_interval_seconds": resource.Int64Attribute{
							Computed:    true,
							Description: "Frequency at which consumer group offset checkpoints are emitted (default: 60, every minute).",
							Optional:    true,
						},
						"refresh_groups_enabled": resource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically check for new consumer groups. Defaults to 'true'.",
							Optional:    true,
						},
						"refresh_groups_interval_seconds": resource.Int64Attribute{
							Computed:    true,
							Description: "Frequency of consumer group refresh in seconds. Defaults to 600 seconds (10 minutes).",
							Optional:    true,
						},
						"refresh_topics_enabled": resource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically check for new topics and partitions. Defaults to 'true'.",
							Optional:    true,
						},
						"refresh_topics_interval_seconds": resource.Int64Attribute{
							Computed:    true,
							Description: "Frequency of topic and partitions refresh in seconds. Defaults to 600 seconds (10 minutes).",
							Optional:    true,
						},
						"sync_group_offsets_enabled": resource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically write the translated offsets of replicated consumer groups (in the source cluster) to __consumer_offsets topic in target cluster, as long as no active consumers in that group are connected to the target cluster.",
							Optional:    true,
						},
						"sync_group_offsets_interval_seconds": resource.Int64Attribute{
							Computed:    true,
							Description: "Frequency at which consumer group offsets are synced (default: 60, every minute).",
							Optional:    true,
						},
						"sync_topic_configs_enabled": resource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically configure remote topics to match their corresponding upstream topics.",
							Optional:    true,
						},
						"tasks_max_per_cpu": resource.Int64Attribute{
							Computed:    true,
							Default:     int64default.StaticInt64(1),
							Description: "'tasks.max' is set to this multiplied by the number of CPUs in the service. The default value is `1`.",
							Optional:    true,
						},
					}},
				},
			},
		},
		Validators: []validator.Set{setvalidator.SizeAtMost(1)},
	}
}

// NewDataSourceSchema returns datasource schema
func NewDataSourceSchema() datasource.SetNestedBlock {
	return datasource.SetNestedBlock{
		Description: "KafkaMirrormaker user configurable settings",
		NestedObject: datasource.NestedBlockObject{
			Attributes: map[string]datasource.Attribute{
				"additional_backup_regions": datasource.SetAttribute{
					Computed:    true,
					Description: "Additional Cloud Regions for Backup Replication.",
					ElementType: types.StringType,
					Validators:  []validator.Set{setvalidator.SizeAtMost(1)},
				},
				"static_ips": datasource.BoolAttribute{
					Computed:    true,
					Description: "Use static public IP addresses.",
				},
			},
			Blocks: map[string]datasource.Block{
				"ip_filter": datasource.SetNestedBlock{
					Description: "Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"description": datasource.StringAttribute{
							Computed:    true,
							Description: "Description for IP filter list entry.",
						},
						"network": datasource.StringAttribute{
							Computed:    true,
							Description: "CIDR address block.",
						},
					}},
					Validators: []validator.Set{setvalidator.SizeAtMost(1024)},
				},
				"kafka_mirrormaker": datasource.SetNestedBlock{
					Description: "Kafka MirrorMaker configuration values",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"emit_checkpoints_enabled": datasource.BoolAttribute{
							Computed:    true,
							Description: "Whether to emit consumer group offset checkpoints to target cluster periodically (default: true).",
						},
						"emit_checkpoints_interval_seconds": datasource.Int64Attribute{
							Computed:    true,
							Description: "Frequency at which consumer group offset checkpoints are emitted (default: 60, every minute).",
						},
						"refresh_groups_enabled": datasource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically check for new consumer groups. Defaults to 'true'.",
						},
						"refresh_groups_interval_seconds": datasource.Int64Attribute{
							Computed:    true,
							Description: "Frequency of consumer group refresh in seconds. Defaults to 600 seconds (10 minutes).",
						},
						"refresh_topics_enabled": datasource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically check for new topics and partitions. Defaults to 'true'.",
						},
						"refresh_topics_interval_seconds": datasource.Int64Attribute{
							Computed:    true,
							Description: "Frequency of topic and partitions refresh in seconds. Defaults to 600 seconds (10 minutes).",
						},
						"sync_group_offsets_enabled": datasource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically write the translated offsets of replicated consumer groups (in the source cluster) to __consumer_offsets topic in target cluster, as long as no active consumers in that group are connected to the target cluster.",
						},
						"sync_group_offsets_interval_seconds": datasource.Int64Attribute{
							Computed:    true,
							Description: "Frequency at which consumer group offsets are synced (default: 60, every minute).",
						},
						"sync_topic_configs_enabled": datasource.BoolAttribute{
							Computed:    true,
							Description: "Whether to periodically configure remote topics to match their corresponding upstream topics.",
						},
						"tasks_max_per_cpu": datasource.Int64Attribute{
							Computed:    true,
							Description: "'tasks.max' is set to this multiplied by the number of CPUs in the service. The default value is `1`.",
						},
					}},
				},
			},
		},
		Validators: []validator.Set{setvalidator.SizeAtMost(1)},
	}
}

// tfoUserConfig KafkaMirrormaker user configurable settings
type tfoUserConfig struct {
	AdditionalBackupRegions types.Set  `tfsdk:"additional_backup_regions"`
	IpFilter                types.Set  `tfsdk:"ip_filter"`
	KafkaMirrormaker        types.Set  `tfsdk:"kafka_mirrormaker"`
	StaticIps               types.Bool `tfsdk:"static_ips"`
}

// dtoUserConfig request/response object
type dtoUserConfig struct {
	AdditionalBackupRegions []string             `groups:"create,update" json:"additional_backup_regions,omitempty"`
	IpFilter                []*dtoIpFilter       `groups:"create,update" json:"ip_filter,omitempty"`
	KafkaMirrormaker        *dtoKafkaMirrormaker `groups:"create,update" json:"kafka_mirrormaker,omitempty"`
	StaticIps               *bool                `groups:"create,update" json:"static_ips,omitempty"`
}

// expandUserConfig expands tf object into dto object
func expandUserConfig(ctx context.Context, diags diag.Diagnostics, o *tfoUserConfig) *dtoUserConfig {
	additionalBackupRegionsVar := schemautil.ExpandSet[string](ctx, diags, o.AdditionalBackupRegions)
	if diags.HasError() {
		return nil
	}
	ipFilterVar := schemautil.ExpandSetNested(ctx, diags, expandIpFilter, o.IpFilter)
	if diags.HasError() {
		return nil
	}
	kafkaMirrormakerVar := schemautil.ExpandSetBlockNested(ctx, diags, expandKafkaMirrormaker, o.KafkaMirrormaker)
	if diags.HasError() {
		return nil
	}
	return &dtoUserConfig{
		AdditionalBackupRegions: additionalBackupRegionsVar,
		IpFilter:                ipFilterVar,
		KafkaMirrormaker:        kafkaMirrormakerVar,
		StaticIps:               schemautil.ValueBoolPointer(o.StaticIps),
	}
}

// flattenUserConfig flattens dto object into tf object
func flattenUserConfig(ctx context.Context, diags diag.Diagnostics, o *dtoUserConfig) *tfoUserConfig {
	additionalBackupRegionsVar, d := types.SetValueFrom(ctx, types.StringType, o.AdditionalBackupRegions)
	diags.Append(d...)
	if diags.HasError() {
		return nil
	}
	ipFilterVar := schemautil.FlattenSetNested(ctx, diags, flattenIpFilter, o.IpFilter, ipFilterAttrs)
	if diags.HasError() {
		return nil
	}
	kafkaMirrormakerVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenKafkaMirrormaker, o.KafkaMirrormaker, kafkaMirrormakerAttrs)
	if diags.HasError() {
		return nil
	}
	return &tfoUserConfig{
		AdditionalBackupRegions: additionalBackupRegionsVar,
		IpFilter:                ipFilterVar,
		KafkaMirrormaker:        kafkaMirrormakerVar,
		StaticIps:               types.BoolPointerValue(o.StaticIps),
	}
}

var userConfigAttrs = map[string]attr.Type{
	"additional_backup_regions": types.SetType{ElemType: types.StringType},
	"ip_filter":                 types.SetType{ElemType: types.ObjectType{AttrTypes: ipFilterAttrs}},
	"kafka_mirrormaker":         types.SetType{ElemType: types.ObjectType{AttrTypes: kafkaMirrormakerAttrs}},
	"static_ips":                types.BoolType,
}

// tfoIpFilter CIDR address block, either as a string, or in a dict with an optional description field
type tfoIpFilter struct {
	Description types.String `tfsdk:"description"`
	Network     types.String `tfsdk:"network"`
}

// dtoIpFilter request/response object
type dtoIpFilter struct {
	Description *string `groups:"create,update" json:"description,omitempty"`
	Network     string  `groups:"create,update" json:"network"`
}

func (d *dtoIpFilter) UnmarshalJSON(data []byte) error {
	var s string
	err := json.Unmarshal(data, &s)
	if err == nil {
		d.Network = s
		return nil
	}

	type obj dtoIpFilter
	o := &struct {
		Description *string `groups:"create,update" json:"description,omitempty"`
		Network     string  `groups:"create,update" json:"network"`
	}{}
	err = json.Unmarshal(data, o)
	if err != nil {
		return err
	}

	d.Description = o.Description
	d.Network = o.Network
	return nil
}

// expandIpFilter expands tf object into dto object
func expandIpFilter(ctx context.Context, diags diag.Diagnostics, o *tfoIpFilter) *dtoIpFilter {
	return &dtoIpFilter{
		Description: schemautil.ValueStringPointer(o.Description),
		Network:     o.Network.ValueString(),
	}
}

// flattenIpFilter flattens dto object into tf object
func flattenIpFilter(ctx context.Context, diags diag.Diagnostics, o *dtoIpFilter) *tfoIpFilter {
	return &tfoIpFilter{
		Description: types.StringPointerValue(o.Description),
		Network:     types.StringValue(o.Network),
	}
}

var ipFilterAttrs = map[string]attr.Type{
	"description": types.StringType,
	"network":     types.StringType,
}

// tfoKafkaMirrormaker Kafka MirrorMaker configuration values
type tfoKafkaMirrormaker struct {
	EmitCheckpointsEnabled          types.Bool  `tfsdk:"emit_checkpoints_enabled"`
	EmitCheckpointsIntervalSeconds  types.Int64 `tfsdk:"emit_checkpoints_interval_seconds"`
	RefreshGroupsEnabled            types.Bool  `tfsdk:"refresh_groups_enabled"`
	RefreshGroupsIntervalSeconds    types.Int64 `tfsdk:"refresh_groups_interval_seconds"`
	RefreshTopicsEnabled            types.Bool  `tfsdk:"refresh_topics_enabled"`
	RefreshTopicsIntervalSeconds    types.Int64 `tfsdk:"refresh_topics_interval_seconds"`
	SyncGroupOffsetsEnabled         types.Bool  `tfsdk:"sync_group_offsets_enabled"`
	SyncGroupOffsetsIntervalSeconds types.Int64 `tfsdk:"sync_group_offsets_interval_seconds"`
	SyncTopicConfigsEnabled         types.Bool  `tfsdk:"sync_topic_configs_enabled"`
	TasksMaxPerCpu                  types.Int64 `tfsdk:"tasks_max_per_cpu"`
}

// dtoKafkaMirrormaker request/response object
type dtoKafkaMirrormaker struct {
	EmitCheckpointsEnabled          *bool  `groups:"create,update" json:"emit_checkpoints_enabled,omitempty"`
	EmitCheckpointsIntervalSeconds  *int64 `groups:"create,update" json:"emit_checkpoints_interval_seconds,omitempty"`
	RefreshGroupsEnabled            *bool  `groups:"create,update" json:"refresh_groups_enabled,omitempty"`
	RefreshGroupsIntervalSeconds    *int64 `groups:"create,update" json:"refresh_groups_interval_seconds,omitempty"`
	RefreshTopicsEnabled            *bool  `groups:"create,update" json:"refresh_topics_enabled,omitempty"`
	RefreshTopicsIntervalSeconds    *int64 `groups:"create,update" json:"refresh_topics_interval_seconds,omitempty"`
	SyncGroupOffsetsEnabled         *bool  `groups:"create,update" json:"sync_group_offsets_enabled,omitempty"`
	SyncGroupOffsetsIntervalSeconds *int64 `groups:"create,update" json:"sync_group_offsets_interval_seconds,omitempty"`
	SyncTopicConfigsEnabled         *bool  `groups:"create,update" json:"sync_topic_configs_enabled,omitempty"`
	TasksMaxPerCpu                  *int64 `groups:"create,update" json:"tasks_max_per_cpu,omitempty"`
}

// expandKafkaMirrormaker expands tf object into dto object
func expandKafkaMirrormaker(ctx context.Context, diags diag.Diagnostics, o *tfoKafkaMirrormaker) *dtoKafkaMirrormaker {
	return &dtoKafkaMirrormaker{
		EmitCheckpointsEnabled:          schemautil.ValueBoolPointer(o.EmitCheckpointsEnabled),
		EmitCheckpointsIntervalSeconds:  schemautil.ValueInt64Pointer(o.EmitCheckpointsIntervalSeconds),
		RefreshGroupsEnabled:            schemautil.ValueBoolPointer(o.RefreshGroupsEnabled),
		RefreshGroupsIntervalSeconds:    schemautil.ValueInt64Pointer(o.RefreshGroupsIntervalSeconds),
		RefreshTopicsEnabled:            schemautil.ValueBoolPointer(o.RefreshTopicsEnabled),
		RefreshTopicsIntervalSeconds:    schemautil.ValueInt64Pointer(o.RefreshTopicsIntervalSeconds),
		SyncGroupOffsetsEnabled:         schemautil.ValueBoolPointer(o.SyncGroupOffsetsEnabled),
		SyncGroupOffsetsIntervalSeconds: schemautil.ValueInt64Pointer(o.SyncGroupOffsetsIntervalSeconds),
		SyncTopicConfigsEnabled:         schemautil.ValueBoolPointer(o.SyncTopicConfigsEnabled),
		TasksMaxPerCpu:                  schemautil.ValueInt64Pointer(o.TasksMaxPerCpu),
	}
}

// flattenKafkaMirrormaker flattens dto object into tf object
func flattenKafkaMirrormaker(ctx context.Context, diags diag.Diagnostics, o *dtoKafkaMirrormaker) *tfoKafkaMirrormaker {
	return &tfoKafkaMirrormaker{
		EmitCheckpointsEnabled:          types.BoolPointerValue(o.EmitCheckpointsEnabled),
		EmitCheckpointsIntervalSeconds:  types.Int64PointerValue(o.EmitCheckpointsIntervalSeconds),
		RefreshGroupsEnabled:            types.BoolPointerValue(o.RefreshGroupsEnabled),
		RefreshGroupsIntervalSeconds:    types.Int64PointerValue(o.RefreshGroupsIntervalSeconds),
		RefreshTopicsEnabled:            types.BoolPointerValue(o.RefreshTopicsEnabled),
		RefreshTopicsIntervalSeconds:    types.Int64PointerValue(o.RefreshTopicsIntervalSeconds),
		SyncGroupOffsetsEnabled:         types.BoolPointerValue(o.SyncGroupOffsetsEnabled),
		SyncGroupOffsetsIntervalSeconds: types.Int64PointerValue(o.SyncGroupOffsetsIntervalSeconds),
		SyncTopicConfigsEnabled:         types.BoolPointerValue(o.SyncTopicConfigsEnabled),
		TasksMaxPerCpu:                  types.Int64PointerValue(o.TasksMaxPerCpu),
	}
}

var kafkaMirrormakerAttrs = map[string]attr.Type{
	"emit_checkpoints_enabled":            types.BoolType,
	"emit_checkpoints_interval_seconds":   types.Int64Type,
	"refresh_groups_enabled":              types.BoolType,
	"refresh_groups_interval_seconds":     types.Int64Type,
	"refresh_topics_enabled":              types.BoolType,
	"refresh_topics_interval_seconds":     types.Int64Type,
	"sync_group_offsets_enabled":          types.BoolType,
	"sync_group_offsets_interval_seconds": types.Int64Type,
	"sync_topic_configs_enabled":          types.BoolType,
	"tasks_max_per_cpu":                   types.Int64Type,
}

// Expand public function that converts tf object into dto
func Expand(ctx context.Context, diags diag.Diagnostics, set types.Set) *dtoUserConfig {
	return schemautil.ExpandSetBlockNested[tfoUserConfig, dtoUserConfig](ctx, diags, expandUserConfig, set)
}

// Flatten public function that converts dto into tf object
func Flatten(ctx context.Context, diags diag.Diagnostics, m map[string]any) types.Set {
	o := new(dtoUserConfig)
	err := schemautil.MapToDTO(m, o)
	if err != nil {
		diags.AddError("Failed to marshal map user config to dto", err.Error())
		return types.SetNull(types.ObjectType{AttrTypes: userConfigAttrs})
	}
	return schemautil.FlattenSetBlockNested[dtoUserConfig, tfoUserConfig](ctx, diags, flattenUserConfig, o, userConfigAttrs)
}
