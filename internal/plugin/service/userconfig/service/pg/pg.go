// Code generated by user config generator. DO NOT EDIT.

package pg

import (
	"context"
	"encoding/json"

	setvalidator "github.com/hashicorp/terraform-plugin-framework-validators/setvalidator"
	attr "github.com/hashicorp/terraform-plugin-framework/attr"
	datasource "github.com/hashicorp/terraform-plugin-framework/datasource/schema"
	diag "github.com/hashicorp/terraform-plugin-framework/diag"
	resource "github.com/hashicorp/terraform-plugin-framework/resource/schema"
	booldefault "github.com/hashicorp/terraform-plugin-framework/resource/schema/booldefault"
	int64default "github.com/hashicorp/terraform-plugin-framework/resource/schema/int64default"
	validator "github.com/hashicorp/terraform-plugin-framework/schema/validator"
	types "github.com/hashicorp/terraform-plugin-framework/types"

	schemautil "github.com/aiven/terraform-provider-aiven/internal/schemautil"
)

// NewResourceSchema returns resource schema
func NewResourceSchema() resource.SetNestedBlock {
	return resource.SetNestedBlock{
		Description: "Pg user configurable settings",
		NestedObject: resource.NestedBlockObject{
			Attributes: map[string]resource.Attribute{
				"additional_backup_regions": resource.SetAttribute{
					Computed:    true,
					Description: "Additional Cloud Regions for Backup Replication.",
					ElementType: types.StringType,
					Optional:    true,
					Validators:  []validator.Set{setvalidator.SizeAtMost(1)},
				},
				"admin_password": resource.StringAttribute{
					Computed:    true,
					Description: "Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.",
					Optional:    true,
				},
				"admin_username": resource.StringAttribute{
					Computed:    true,
					Description: "Custom username for admin user. This must be set only when a new service is being created.",
					Optional:    true,
				},
				"backup_hour": resource.Int64Attribute{
					Computed:    true,
					Description: "The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed.",
					Optional:    true,
				},
				"backup_minute": resource.Int64Attribute{
					Computed:    true,
					Description: "The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed.",
					Optional:    true,
				},
				"enable_ipv6": resource.BoolAttribute{
					Computed:    true,
					Description: "Register AAAA DNS records for the service, and allow IPv6 packets to service ports.",
					Optional:    true,
				},
				"pg_read_replica": resource.BoolAttribute{
					Computed:    true,
					Description: "Should the service which is being forked be a read replica (deprecated, use read_replica service integration instead).",
					Optional:    true,
				},
				"pg_service_to_fork_from": resource.StringAttribute{
					Computed:    true,
					Description: "Name of the PG Service from which to fork (deprecated, use service_to_fork_from). This has effect only when a new service is being created.",
					Optional:    true,
				},
				"pg_stat_monitor_enable": resource.BoolAttribute{
					Computed:    true,
					Default:     booldefault.StaticBool(false),
					Description: "Enable the pg_stat_monitor extension. Enabling this extension will cause the cluster to be restarted.When this extension is enabled, pg_stat_statements results for utility commands are unreliable. The default value is `false`.",
					Optional:    true,
				},
				"pg_version": resource.StringAttribute{
					Computed:    true,
					Description: "PostgreSQL major version.",
					Optional:    true,
				},
				"project_to_fork_from": resource.StringAttribute{
					Computed:    true,
					Description: "Name of another project to fork a service from. This has effect only when a new service is being created.",
					Optional:    true,
				},
				"recovery_target_time": resource.StringAttribute{
					Computed:    true,
					Description: "Recovery target time when forking a service. This has effect only when a new service is being created.",
					Optional:    true,
				},
				"service_to_fork_from": resource.StringAttribute{
					Computed:    true,
					Description: "Name of another service to fork from. This has effect only when a new service is being created.",
					Optional:    true,
				},
				"shared_buffers_percentage": resource.Float64Attribute{
					Computed:    true,
					Description: "Percentage of total RAM that the database server uses for shared memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts the shared_buffers configuration value.",
					Optional:    true,
				},
				"static_ips": resource.BoolAttribute{
					Computed:    true,
					Description: "Use static public IP addresses.",
					Optional:    true,
				},
				"synchronous_replication": resource.StringAttribute{
					Computed:    true,
					Description: "Synchronous replication type. Note that the service plan also needs to support synchronous replication.",
					Optional:    true,
				},
				"variant": resource.StringAttribute{
					Computed:    true,
					Description: "Variant of the PostgreSQL service, may affect the features that are exposed by default.",
					Optional:    true,
				},
				"work_mem": resource.Int64Attribute{
					Computed:    true,
					Description: "Sets the maximum amount of memory to be used by a query operation (such as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of total RAM (up to 32MB).",
					Optional:    true,
				},
			},
			Blocks: map[string]resource.Block{
				"ip_filter": resource.SetNestedBlock{
					Description: "Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"description": resource.StringAttribute{
							Computed:    true,
							Description: "Description for IP filter list entry.",
							Optional:    true,
						},
						"network": resource.StringAttribute{
							Description: "CIDR address block.",
							Required:    true,
						},
					}},
					Validators: []validator.Set{setvalidator.SizeAtMost(1024)},
				},
				"migration": resource.SetNestedBlock{
					Description: "Migrate data from existing server",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"dbname": resource.StringAttribute{
							Computed:    true,
							Description: "Database name for bootstrapping the initial connection.",
							Optional:    true,
						},
						"host": resource.StringAttribute{
							Description: "Hostname or IP address of the server where to migrate data from.",
							Required:    true,
						},
						"ignore_dbs": resource.StringAttribute{
							Computed:    true,
							Description: "Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment).",
							Optional:    true,
						},
						"method": resource.StringAttribute{
							Computed:    true,
							Description: "The migration method to be used (currently supported only by Redis, MySQL and PostgreSQL service types).",
							Optional:    true,
						},
						"password": resource.StringAttribute{
							Computed:    true,
							Description: "Password for authentication with the server where to migrate data from.",
							Optional:    true,
						},
						"port": resource.Int64Attribute{
							Description: "Port number of the server where to migrate data from.",
							Required:    true,
						},
						"ssl": resource.BoolAttribute{
							Computed:    true,
							Default:     booldefault.StaticBool(true),
							Description: "The server where to migrate data from is secured with SSL. The default value is `true`.",
							Optional:    true,
						},
						"username": resource.StringAttribute{
							Computed:    true,
							Description: "User name for authentication with the server where to migrate data from.",
							Optional:    true,
						},
					}},
				},
				"pg": resource.SetNestedBlock{
					Description: "postgresql.conf configuration values",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"autovacuum_analyze_scale_factor": resource.Float64Attribute{
							Computed:    true,
							Description: "Specifies a fraction of the table size to add to autovacuum_analyze_threshold when deciding whether to trigger an ANALYZE. The default is 0.2 (20% of table size).",
							Optional:    true,
						},
						"autovacuum_analyze_threshold": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an  ANALYZE in any one table. The default is 50 tuples.",
							Optional:    true,
						},
						"autovacuum_freeze_max_age": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted.",
							Optional:    true,
						},
						"autovacuum_max_workers": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.",
							Optional:    true,
						},
						"autovacuum_naptime": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the minimum delay between autovacuum runs on any given database. The delay is measured in seconds, and the default is one minute.",
							Optional:    true,
						},
						"autovacuum_vacuum_cost_delay": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuum_cost_delay value will be used. The default value is 20 milliseconds.",
							Optional:    true,
						},
						"autovacuum_vacuum_cost_limit": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum_cost_limit value will be used.",
							Optional:    true,
						},
						"autovacuum_vacuum_scale_factor": resource.Float64Attribute{
							Computed:    true,
							Description: "Specifies a fraction of the table size to add to autovacuum_vacuum_threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).",
							Optional:    true,
						},
						"autovacuum_vacuum_threshold": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples.",
							Optional:    true,
						},
						"bgwriter_delay": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the delay between activity rounds for the background writer in milliseconds. Default is 200.",
							Optional:    true,
						},
						"bgwriter_flush_after": resource.Int64Attribute{
							Computed:    true,
							Description: "Whenever more than bgwriter_flush_after bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage. Specified in kilobytes, default is 512. Setting of 0 disables forced writeback.",
							Optional:    true,
						},
						"bgwriter_lru_maxpages": resource.Int64Attribute{
							Computed:    true,
							Description: "In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing. Default is 100.",
							Optional:    true,
						},
						"bgwriter_lru_multiplier": resource.Float64Attribute{
							Computed:    true,
							Description: "The average recent need for new buffers is multiplied by bgwriter_lru_multiplier to arrive at an estimate of the number that will be needed during the next round, (up to bgwriter_lru_maxpages). 1.0 represents a “just in time” policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0.",
							Optional:    true,
						},
						"deadlock_timeout": resource.Int64Attribute{
							Computed:    true,
							Description: "This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition.",
							Optional:    true,
						},
						"default_toast_compression": resource.StringAttribute{
							Computed:    true,
							Description: "Specifies the default TOAST compression method for values of compressible columns (the default is lz4).",
							Optional:    true,
						},
						"idle_in_transaction_session_timeout": resource.Int64Attribute{
							Computed:    true,
							Description: "Time out sessions with open transactions after this number of milliseconds.",
							Optional:    true,
						},
						"jit": resource.BoolAttribute{
							Computed:    true,
							Description: "Controls system-wide use of Just-in-Time Compilation (JIT).",
							Optional:    true,
						},
						"log_autovacuum_min_duration": resource.Int64Attribute{
							Computed:    true,
							Description: "Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions.",
							Optional:    true,
						},
						"log_error_verbosity": resource.StringAttribute{
							Computed:    true,
							Description: "Controls the amount of detail written in the server log for each message that is logged.",
							Optional:    true,
						},
						"log_line_prefix": resource.StringAttribute{
							Computed:    true,
							Description: "Choose from one of the available log-formats. These can support popular log analyzers like pgbadger, pganalyze etc.",
							Optional:    true,
						},
						"log_min_duration_statement": resource.Int64Attribute{
							Computed:    true,
							Description: "Log statements that take more than this number of milliseconds to run, -1 disables.",
							Optional:    true,
						},
						"log_temp_files": resource.Int64Attribute{
							Computed:    true,
							Description: "Log statements for each temporary file created larger than this number of kilobytes, -1 disables.",
							Optional:    true,
						},
						"max_files_per_process": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum number of files that can be open per process.",
							Optional:    true,
						},
						"max_locks_per_transaction": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum locks per transaction.",
							Optional:    true,
						},
						"max_logical_replication_workers": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum logical replication workers (taken from the pool of max_parallel_workers).",
							Optional:    true,
						},
						"max_parallel_workers": resource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of workers that the system can support for parallel queries.",
							Optional:    true,
						},
						"max_parallel_workers_per_gather": resource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of workers that can be started by a single Gather or Gather Merge node.",
							Optional:    true,
						},
						"max_pred_locks_per_transaction": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum predicate locks per transaction.",
							Optional:    true,
						},
						"max_prepared_transactions": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum prepared transactions.",
							Optional:    true,
						},
						"max_replication_slots": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum replication slots.",
							Optional:    true,
						},
						"max_slot_wal_keep_size": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum WAL size (MB) reserved for replication slots. Default is -1 (unlimited). wal_keep_size minimum WAL size setting takes precedence over this.",
							Optional:    true,
						},
						"max_stack_depth": resource.Int64Attribute{
							Computed:    true,
							Description: "Maximum depth of the stack in bytes.",
							Optional:    true,
						},
						"max_standby_archive_delay": resource.Int64Attribute{
							Computed:    true,
							Description: "Max standby archive delay in milliseconds.",
							Optional:    true,
						},
						"max_standby_streaming_delay": resource.Int64Attribute{
							Computed:    true,
							Description: "Max standby streaming delay in milliseconds.",
							Optional:    true,
						},
						"max_wal_senders": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum WAL senders.",
							Optional:    true,
						},
						"max_worker_processes": resource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of background processes that the system can support.",
							Optional:    true,
						},
						"pg_partman_bgw__interval": resource.Int64Attribute{
							Computed:    true,
							Description: "Sets the time interval to run pg_partman's scheduled tasks.",
							Optional:    true,
						},
						"pg_partman_bgw__role": resource.StringAttribute{
							Computed:    true,
							Description: "Controls which role to use for pg_partman's scheduled background tasks.",
							Optional:    true,
						},
						"pg_stat_monitor__pgsm_enable_query_plan": resource.BoolAttribute{
							Computed:    true,
							Description: "Enables or disables query plan monitoring.",
							Optional:    true,
						},
						"pg_stat_monitor__pgsm_max_buckets": resource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of buckets .",
							Optional:    true,
						},
						"pg_stat_statements__track": resource.StringAttribute{
							Computed:    true,
							Description: "Controls which statements are counted. Specify top to track top-level statements (those issued directly by clients), all to also track nested statements (such as statements invoked within functions), or none to disable statement statistics collection. The default value is top.",
							Optional:    true,
						},
						"temp_file_limit": resource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL temporary file limit in KiB, -1 for unlimited.",
							Optional:    true,
						},
						"timezone": resource.StringAttribute{
							Computed:    true,
							Description: "PostgreSQL service timezone.",
							Optional:    true,
						},
						"track_activity_query_size": resource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the number of bytes reserved to track the currently executing command for each active session.",
							Optional:    true,
						},
						"track_commit_timestamp": resource.StringAttribute{
							Computed:    true,
							Description: "Record commit time of transactions.",
							Optional:    true,
						},
						"track_functions": resource.StringAttribute{
							Computed:    true,
							Description: "Enables tracking of function call counts and time used.",
							Optional:    true,
						},
						"track_io_timing": resource.StringAttribute{
							Computed:    true,
							Description: "Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms.",
							Optional:    true,
						},
						"wal_sender_timeout": resource.Int64Attribute{
							Computed:    true,
							Description: "Terminate replication connections that are inactive for longer than this amount of time, in milliseconds. Setting this value to zero disables the timeout.",
							Optional:    true,
						},
						"wal_writer_delay": resource.Int64Attribute{
							Computed:    true,
							Description: "WAL flush interval in milliseconds. Note that setting this value to lower than the default 200ms may negatively impact performance.",
							Optional:    true,
						},
					}},
				},
				"pgbouncer": resource.SetNestedBlock{
					Description: "PGBouncer connection pooling settings",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"autodb_idle_timeout": resource.Int64Attribute{
							Computed:    true,
							Description: "If the automatically created database pools have been unused this many seconds, they are freed. If 0 then timeout is disabled. [seconds].",
							Optional:    true,
						},
						"autodb_max_db_connections": resource.Int64Attribute{
							Computed:    true,
							Description: "Do not allow more than this many server connections per database (regardless of user). Setting it to 0 means unlimited.",
							Optional:    true,
						},
						"autodb_pool_mode": resource.StringAttribute{
							Computed:    true,
							Description: "PGBouncer pool mode.",
							Optional:    true,
						},
						"autodb_pool_size": resource.Int64Attribute{
							Computed:    true,
							Description: "If non-zero then create automatically a pool of that size per user when a pool doesn't exist.",
							Optional:    true,
						},
						"ignore_startup_parameters": resource.SetAttribute{
							Computed:    true,
							Description: "List of parameters to ignore when given in startup packet.",
							ElementType: types.StringType,
							Optional:    true,
							Validators:  []validator.Set{setvalidator.SizeAtMost(32)},
						},
						"min_pool_size": resource.Int64Attribute{
							Computed:    true,
							Description: "Add more server connections to pool if below this number. Improves behavior when usual load comes suddenly back after period of total inactivity. The value is effectively capped at the pool size.",
							Optional:    true,
						},
						"server_idle_timeout": resource.Int64Attribute{
							Computed:    true,
							Description: "If a server connection has been idle more than this many seconds it will be dropped. If 0 then timeout is disabled. [seconds].",
							Optional:    true,
						},
						"server_lifetime": resource.Int64Attribute{
							Computed:    true,
							Description: "The pooler will close an unused server connection that has been connected longer than this. [seconds].",
							Optional:    true,
						},
						"server_reset_query_always": resource.BoolAttribute{
							Computed:    true,
							Description: "Run server_reset_query (DISCARD ALL) in all pooling modes.",
							Optional:    true,
						},
					}},
				},
				"pglookout": resource.SetNestedBlock{
					Description: "PGLookout settings",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{"max_failover_replication_time_lag": resource.Int64Attribute{
						Computed:    true,
						Default:     int64default.StaticInt64(60),
						Description: "Number of seconds of master unavailability before triggering database failover to standby. The default value is `60`.",
						Optional:    true,
					}}},
				},
				"private_access": resource.SetNestedBlock{
					Description: "Allow access to selected service ports from private networks",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"pg": resource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pg with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.",
							Optional:    true,
						},
						"pgbouncer": resource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pgbouncer with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.",
							Optional:    true,
						},
						"prometheus": resource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.",
							Optional:    true,
						},
					}},
				},
				"privatelink_access": resource.SetNestedBlock{
					Description: "Allow access to selected service components through Privatelink",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"pg": resource.BoolAttribute{
							Computed:    true,
							Description: "Enable pg.",
							Optional:    true,
						},
						"pgbouncer": resource.BoolAttribute{
							Computed:    true,
							Description: "Enable pgbouncer.",
							Optional:    true,
						},
						"prometheus": resource.BoolAttribute{
							Computed:    true,
							Description: "Enable prometheus.",
							Optional:    true,
						},
					}},
				},
				"public_access": resource.SetNestedBlock{
					Description: "Allow access to selected service ports from the public Internet",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
						"pg": resource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pg from the public internet for service nodes that are in a project VPC or another type of private network.",
							Optional:    true,
						},
						"pgbouncer": resource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pgbouncer from the public internet for service nodes that are in a project VPC or another type of private network.",
							Optional:    true,
						},
						"prometheus": resource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.",
							Optional:    true,
						},
					}},
				},
				"timescaledb": resource.SetNestedBlock{
					Description: "TimescaleDB extension configuration values",
					NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{"max_background_workers": resource.Int64Attribute{
						Computed:    true,
						Description: "The number of background workers for timescaledb operations. You should configure this setting to the sum of your number of databases and the total number of concurrent background workers you want running at any given point in time.",
						Optional:    true,
					}}},
				},
			},
		},
		Validators: []validator.Set{setvalidator.SizeAtMost(1)},
	}
}

// NewDataSourceSchema returns datasource schema
func NewDataSourceSchema() datasource.SetNestedBlock {
	return datasource.SetNestedBlock{
		Description: "Pg user configurable settings",
		NestedObject: datasource.NestedBlockObject{
			Attributes: map[string]datasource.Attribute{
				"additional_backup_regions": datasource.SetAttribute{
					Computed:    true,
					Description: "Additional Cloud Regions for Backup Replication.",
					ElementType: types.StringType,
					Validators:  []validator.Set{setvalidator.SizeAtMost(1)},
				},
				"admin_password": datasource.StringAttribute{
					Computed:    true,
					Description: "Custom password for admin user. Defaults to random string. This must be set only when a new service is being created.",
				},
				"admin_username": datasource.StringAttribute{
					Computed:    true,
					Description: "Custom username for admin user. This must be set only when a new service is being created.",
				},
				"backup_hour": datasource.Int64Attribute{
					Computed:    true,
					Description: "The hour of day (in UTC) when backup for the service is started. New backup is only started if previous backup has already completed.",
				},
				"backup_minute": datasource.Int64Attribute{
					Computed:    true,
					Description: "The minute of an hour when backup for the service is started. New backup is only started if previous backup has already completed.",
				},
				"enable_ipv6": datasource.BoolAttribute{
					Computed:    true,
					Description: "Register AAAA DNS records for the service, and allow IPv6 packets to service ports.",
				},
				"pg_read_replica": datasource.BoolAttribute{
					Computed:    true,
					Description: "Should the service which is being forked be a read replica (deprecated, use read_replica service integration instead).",
				},
				"pg_service_to_fork_from": datasource.StringAttribute{
					Computed:    true,
					Description: "Name of the PG Service from which to fork (deprecated, use service_to_fork_from). This has effect only when a new service is being created.",
				},
				"pg_stat_monitor_enable": datasource.BoolAttribute{
					Computed:    true,
					Description: "Enable the pg_stat_monitor extension. Enabling this extension will cause the cluster to be restarted.When this extension is enabled, pg_stat_statements results for utility commands are unreliable. The default value is `false`.",
				},
				"pg_version": datasource.StringAttribute{
					Computed:    true,
					Description: "PostgreSQL major version.",
				},
				"project_to_fork_from": datasource.StringAttribute{
					Computed:    true,
					Description: "Name of another project to fork a service from. This has effect only when a new service is being created.",
				},
				"recovery_target_time": datasource.StringAttribute{
					Computed:    true,
					Description: "Recovery target time when forking a service. This has effect only when a new service is being created.",
				},
				"service_to_fork_from": datasource.StringAttribute{
					Computed:    true,
					Description: "Name of another service to fork from. This has effect only when a new service is being created.",
				},
				"shared_buffers_percentage": datasource.Float64Attribute{
					Computed:    true,
					Description: "Percentage of total RAM that the database server uses for shared memory buffers. Valid range is 20-60 (float), which corresponds to 20% - 60%. This setting adjusts the shared_buffers configuration value.",
				},
				"static_ips": datasource.BoolAttribute{
					Computed:    true,
					Description: "Use static public IP addresses.",
				},
				"synchronous_replication": datasource.StringAttribute{
					Computed:    true,
					Description: "Synchronous replication type. Note that the service plan also needs to support synchronous replication.",
				},
				"variant": datasource.StringAttribute{
					Computed:    true,
					Description: "Variant of the PostgreSQL service, may affect the features that are exposed by default.",
				},
				"work_mem": datasource.Int64Attribute{
					Computed:    true,
					Description: "Sets the maximum amount of memory to be used by a query operation (such as a sort or hash table) before writing to temporary disk files, in MB. Default is 1MB + 0.075% of total RAM (up to 32MB).",
				},
			},
			Blocks: map[string]datasource.Block{
				"ip_filter": datasource.SetNestedBlock{
					Description: "Allow incoming connections from CIDR address block, e.g. '10.20.0.0/16'",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"description": datasource.StringAttribute{
							Computed:    true,
							Description: "Description for IP filter list entry.",
						},
						"network": datasource.StringAttribute{
							Computed:    true,
							Description: "CIDR address block.",
						},
					}},
					Validators: []validator.Set{setvalidator.SizeAtMost(1024)},
				},
				"migration": datasource.SetNestedBlock{
					Description: "Migrate data from existing server",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"dbname": datasource.StringAttribute{
							Computed:    true,
							Description: "Database name for bootstrapping the initial connection.",
						},
						"host": datasource.StringAttribute{
							Computed:    true,
							Description: "Hostname or IP address of the server where to migrate data from.",
						},
						"ignore_dbs": datasource.StringAttribute{
							Computed:    true,
							Description: "Comma-separated list of databases, which should be ignored during migration (supported by MySQL and PostgreSQL only at the moment).",
						},
						"method": datasource.StringAttribute{
							Computed:    true,
							Description: "The migration method to be used (currently supported only by Redis, MySQL and PostgreSQL service types).",
						},
						"password": datasource.StringAttribute{
							Computed:    true,
							Description: "Password for authentication with the server where to migrate data from.",
						},
						"port": datasource.Int64Attribute{
							Computed:    true,
							Description: "Port number of the server where to migrate data from.",
						},
						"ssl": datasource.BoolAttribute{
							Computed:    true,
							Description: "The server where to migrate data from is secured with SSL. The default value is `true`.",
						},
						"username": datasource.StringAttribute{
							Computed:    true,
							Description: "User name for authentication with the server where to migrate data from.",
						},
					}},
				},
				"pg": datasource.SetNestedBlock{
					Description: "postgresql.conf configuration values",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"autovacuum_analyze_scale_factor": datasource.Float64Attribute{
							Computed:    true,
							Description: "Specifies a fraction of the table size to add to autovacuum_analyze_threshold when deciding whether to trigger an ANALYZE. The default is 0.2 (20% of table size).",
						},
						"autovacuum_analyze_threshold": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an  ANALYZE in any one table. The default is 50 tuples.",
						},
						"autovacuum_freeze_max_age": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted.",
						},
						"autovacuum_max_workers": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.",
						},
						"autovacuum_naptime": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the minimum delay between autovacuum runs on any given database. The delay is measured in seconds, and the default is one minute.",
						},
						"autovacuum_vacuum_cost_delay": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuum_cost_delay value will be used. The default value is 20 milliseconds.",
						},
						"autovacuum_vacuum_cost_limit": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum_cost_limit value will be used.",
						},
						"autovacuum_vacuum_scale_factor": datasource.Float64Attribute{
							Computed:    true,
							Description: "Specifies a fraction of the table size to add to autovacuum_vacuum_threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).",
						},
						"autovacuum_vacuum_threshold": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples.",
						},
						"bgwriter_delay": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the delay between activity rounds for the background writer in milliseconds. Default is 200.",
						},
						"bgwriter_flush_after": datasource.Int64Attribute{
							Computed:    true,
							Description: "Whenever more than bgwriter_flush_after bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage. Specified in kilobytes, default is 512. Setting of 0 disables forced writeback.",
						},
						"bgwriter_lru_maxpages": datasource.Int64Attribute{
							Computed:    true,
							Description: "In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing. Default is 100.",
						},
						"bgwriter_lru_multiplier": datasource.Float64Attribute{
							Computed:    true,
							Description: "The average recent need for new buffers is multiplied by bgwriter_lru_multiplier to arrive at an estimate of the number that will be needed during the next round, (up to bgwriter_lru_maxpages). 1.0 represents a “just in time” policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0.",
						},
						"deadlock_timeout": datasource.Int64Attribute{
							Computed:    true,
							Description: "This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition.",
						},
						"default_toast_compression": datasource.StringAttribute{
							Computed:    true,
							Description: "Specifies the default TOAST compression method for values of compressible columns (the default is lz4).",
						},
						"idle_in_transaction_session_timeout": datasource.Int64Attribute{
							Computed:    true,
							Description: "Time out sessions with open transactions after this number of milliseconds.",
						},
						"jit": datasource.BoolAttribute{
							Computed:    true,
							Description: "Controls system-wide use of Just-in-Time Compilation (JIT).",
						},
						"log_autovacuum_min_duration": datasource.Int64Attribute{
							Computed:    true,
							Description: "Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions.",
						},
						"log_error_verbosity": datasource.StringAttribute{
							Computed:    true,
							Description: "Controls the amount of detail written in the server log for each message that is logged.",
						},
						"log_line_prefix": datasource.StringAttribute{
							Computed:    true,
							Description: "Choose from one of the available log-formats. These can support popular log analyzers like pgbadger, pganalyze etc.",
						},
						"log_min_duration_statement": datasource.Int64Attribute{
							Computed:    true,
							Description: "Log statements that take more than this number of milliseconds to run, -1 disables.",
						},
						"log_temp_files": datasource.Int64Attribute{
							Computed:    true,
							Description: "Log statements for each temporary file created larger than this number of kilobytes, -1 disables.",
						},
						"max_files_per_process": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum number of files that can be open per process.",
						},
						"max_locks_per_transaction": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum locks per transaction.",
						},
						"max_logical_replication_workers": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum logical replication workers (taken from the pool of max_parallel_workers).",
						},
						"max_parallel_workers": datasource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of workers that the system can support for parallel queries.",
						},
						"max_parallel_workers_per_gather": datasource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of workers that can be started by a single Gather or Gather Merge node.",
						},
						"max_pred_locks_per_transaction": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum predicate locks per transaction.",
						},
						"max_prepared_transactions": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum prepared transactions.",
						},
						"max_replication_slots": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum replication slots.",
						},
						"max_slot_wal_keep_size": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum WAL size (MB) reserved for replication slots. Default is -1 (unlimited). wal_keep_size minimum WAL size setting takes precedence over this.",
						},
						"max_stack_depth": datasource.Int64Attribute{
							Computed:    true,
							Description: "Maximum depth of the stack in bytes.",
						},
						"max_standby_archive_delay": datasource.Int64Attribute{
							Computed:    true,
							Description: "Max standby archive delay in milliseconds.",
						},
						"max_standby_streaming_delay": datasource.Int64Attribute{
							Computed:    true,
							Description: "Max standby streaming delay in milliseconds.",
						},
						"max_wal_senders": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL maximum WAL senders.",
						},
						"max_worker_processes": datasource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of background processes that the system can support.",
						},
						"pg_partman_bgw__interval": datasource.Int64Attribute{
							Computed:    true,
							Description: "Sets the time interval to run pg_partman's scheduled tasks.",
						},
						"pg_partman_bgw__role": datasource.StringAttribute{
							Computed:    true,
							Description: "Controls which role to use for pg_partman's scheduled background tasks.",
						},
						"pg_stat_monitor__pgsm_enable_query_plan": datasource.BoolAttribute{
							Computed:    true,
							Description: "Enables or disables query plan monitoring.",
						},
						"pg_stat_monitor__pgsm_max_buckets": datasource.Int64Attribute{
							Computed:    true,
							Description: "Sets the maximum number of buckets .",
						},
						"pg_stat_statements__track": datasource.StringAttribute{
							Computed:    true,
							Description: "Controls which statements are counted. Specify top to track top-level statements (those issued directly by clients), all to also track nested statements (such as statements invoked within functions), or none to disable statement statistics collection. The default value is top.",
						},
						"temp_file_limit": datasource.Int64Attribute{
							Computed:    true,
							Description: "PostgreSQL temporary file limit in KiB, -1 for unlimited.",
						},
						"timezone": datasource.StringAttribute{
							Computed:    true,
							Description: "PostgreSQL service timezone.",
						},
						"track_activity_query_size": datasource.Int64Attribute{
							Computed:    true,
							Description: "Specifies the number of bytes reserved to track the currently executing command for each active session.",
						},
						"track_commit_timestamp": datasource.StringAttribute{
							Computed:    true,
							Description: "Record commit time of transactions.",
						},
						"track_functions": datasource.StringAttribute{
							Computed:    true,
							Description: "Enables tracking of function call counts and time used.",
						},
						"track_io_timing": datasource.StringAttribute{
							Computed:    true,
							Description: "Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms.",
						},
						"wal_sender_timeout": datasource.Int64Attribute{
							Computed:    true,
							Description: "Terminate replication connections that are inactive for longer than this amount of time, in milliseconds. Setting this value to zero disables the timeout.",
						},
						"wal_writer_delay": datasource.Int64Attribute{
							Computed:    true,
							Description: "WAL flush interval in milliseconds. Note that setting this value to lower than the default 200ms may negatively impact performance.",
						},
					}},
				},
				"pgbouncer": datasource.SetNestedBlock{
					Description: "PGBouncer connection pooling settings",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"autodb_idle_timeout": datasource.Int64Attribute{
							Computed:    true,
							Description: "If the automatically created database pools have been unused this many seconds, they are freed. If 0 then timeout is disabled. [seconds].",
						},
						"autodb_max_db_connections": datasource.Int64Attribute{
							Computed:    true,
							Description: "Do not allow more than this many server connections per database (regardless of user). Setting it to 0 means unlimited.",
						},
						"autodb_pool_mode": datasource.StringAttribute{
							Computed:    true,
							Description: "PGBouncer pool mode.",
						},
						"autodb_pool_size": datasource.Int64Attribute{
							Computed:    true,
							Description: "If non-zero then create automatically a pool of that size per user when a pool doesn't exist.",
						},
						"ignore_startup_parameters": datasource.SetAttribute{
							Computed:    true,
							Description: "List of parameters to ignore when given in startup packet.",
							ElementType: types.StringType,
							Validators:  []validator.Set{setvalidator.SizeAtMost(32)},
						},
						"min_pool_size": datasource.Int64Attribute{
							Computed:    true,
							Description: "Add more server connections to pool if below this number. Improves behavior when usual load comes suddenly back after period of total inactivity. The value is effectively capped at the pool size.",
						},
						"server_idle_timeout": datasource.Int64Attribute{
							Computed:    true,
							Description: "If a server connection has been idle more than this many seconds it will be dropped. If 0 then timeout is disabled. [seconds].",
						},
						"server_lifetime": datasource.Int64Attribute{
							Computed:    true,
							Description: "The pooler will close an unused server connection that has been connected longer than this. [seconds].",
						},
						"server_reset_query_always": datasource.BoolAttribute{
							Computed:    true,
							Description: "Run server_reset_query (DISCARD ALL) in all pooling modes.",
						},
					}},
				},
				"pglookout": datasource.SetNestedBlock{
					Description: "PGLookout settings",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{"max_failover_replication_time_lag": datasource.Int64Attribute{
						Computed:    true,
						Description: "Number of seconds of master unavailability before triggering database failover to standby. The default value is `60`.",
					}}},
				},
				"private_access": datasource.SetNestedBlock{
					Description: "Allow access to selected service ports from private networks",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"pg": datasource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pg with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.",
						},
						"pgbouncer": datasource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pgbouncer with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.",
						},
						"prometheus": datasource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to prometheus with a DNS name that always resolves to the service's private IP addresses. Only available in certain network locations.",
						},
					}},
				},
				"privatelink_access": datasource.SetNestedBlock{
					Description: "Allow access to selected service components through Privatelink",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"pg": datasource.BoolAttribute{
							Computed:    true,
							Description: "Enable pg.",
						},
						"pgbouncer": datasource.BoolAttribute{
							Computed:    true,
							Description: "Enable pgbouncer.",
						},
						"prometheus": datasource.BoolAttribute{
							Computed:    true,
							Description: "Enable prometheus.",
						},
					}},
				},
				"public_access": datasource.SetNestedBlock{
					Description: "Allow access to selected service ports from the public Internet",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
						"pg": datasource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pg from the public internet for service nodes that are in a project VPC or another type of private network.",
						},
						"pgbouncer": datasource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to pgbouncer from the public internet for service nodes that are in a project VPC or another type of private network.",
						},
						"prometheus": datasource.BoolAttribute{
							Computed:    true,
							Description: "Allow clients to connect to prometheus from the public internet for service nodes that are in a project VPC or another type of private network.",
						},
					}},
				},
				"timescaledb": datasource.SetNestedBlock{
					Description: "TimescaleDB extension configuration values",
					NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{"max_background_workers": datasource.Int64Attribute{
						Computed:    true,
						Description: "The number of background workers for timescaledb operations. You should configure this setting to the sum of your number of databases and the total number of concurrent background workers you want running at any given point in time.",
					}}},
				},
			},
		},
		Validators: []validator.Set{setvalidator.SizeAtMost(1)},
	}
}

// tfoUserConfig Pg user configurable settings
type tfoUserConfig struct {
	AdditionalBackupRegions types.Set     `tfsdk:"additional_backup_regions"`
	AdminPassword           types.String  `tfsdk:"admin_password"`
	AdminUsername           types.String  `tfsdk:"admin_username"`
	BackupHour              types.Int64   `tfsdk:"backup_hour"`
	BackupMinute            types.Int64   `tfsdk:"backup_minute"`
	EnableIpv6              types.Bool    `tfsdk:"enable_ipv6"`
	IpFilter                types.Set     `tfsdk:"ip_filter"`
	Migration               types.Set     `tfsdk:"migration"`
	Pg                      types.Set     `tfsdk:"pg"`
	PgReadReplica           types.Bool    `tfsdk:"pg_read_replica"`
	PgServiceToForkFrom     types.String  `tfsdk:"pg_service_to_fork_from"`
	PgStatMonitorEnable     types.Bool    `tfsdk:"pg_stat_monitor_enable"`
	PgVersion               types.String  `tfsdk:"pg_version"`
	Pgbouncer               types.Set     `tfsdk:"pgbouncer"`
	Pglookout               types.Set     `tfsdk:"pglookout"`
	PrivateAccess           types.Set     `tfsdk:"private_access"`
	PrivatelinkAccess       types.Set     `tfsdk:"privatelink_access"`
	ProjectToForkFrom       types.String  `tfsdk:"project_to_fork_from"`
	PublicAccess            types.Set     `tfsdk:"public_access"`
	RecoveryTargetTime      types.String  `tfsdk:"recovery_target_time"`
	ServiceToForkFrom       types.String  `tfsdk:"service_to_fork_from"`
	SharedBuffersPercentage types.Float64 `tfsdk:"shared_buffers_percentage"`
	StaticIps               types.Bool    `tfsdk:"static_ips"`
	SynchronousReplication  types.String  `tfsdk:"synchronous_replication"`
	Timescaledb             types.Set     `tfsdk:"timescaledb"`
	Variant                 types.String  `tfsdk:"variant"`
	WorkMem                 types.Int64   `tfsdk:"work_mem"`
}

// dtoUserConfig request/response object
type dtoUserConfig struct {
	AdditionalBackupRegions []string              `groups:"create,update" json:"additional_backup_regions,omitempty"`
	AdminPassword           *string               `groups:"create" json:"admin_password,omitempty"`
	AdminUsername           *string               `groups:"create" json:"admin_username,omitempty"`
	BackupHour              *int64                `groups:"create,update" json:"backup_hour,omitempty"`
	BackupMinute            *int64                `groups:"create,update" json:"backup_minute,omitempty"`
	EnableIpv6              *bool                 `groups:"create,update" json:"enable_ipv6,omitempty"`
	IpFilter                []*dtoIpFilter        `groups:"create,update" json:"ip_filter,omitempty"`
	Migration               *dtoMigration         `groups:"create,update" json:"migration,omitempty"`
	Pg                      *dtoPg                `groups:"create,update" json:"pg,omitempty"`
	PgReadReplica           *bool                 `groups:"create,update" json:"pg_read_replica,omitempty"`
	PgServiceToForkFrom     *string               `groups:"create" json:"pg_service_to_fork_from,omitempty"`
	PgStatMonitorEnable     *bool                 `groups:"create,update" json:"pg_stat_monitor_enable,omitempty"`
	PgVersion               *string               `groups:"create,update" json:"pg_version,omitempty"`
	Pgbouncer               *dtoPgbouncer         `groups:"create,update" json:"pgbouncer,omitempty"`
	Pglookout               *dtoPglookout         `groups:"create,update" json:"pglookout,omitempty"`
	PrivateAccess           *dtoPrivateAccess     `groups:"create,update" json:"private_access,omitempty"`
	PrivatelinkAccess       *dtoPrivatelinkAccess `groups:"create,update" json:"privatelink_access,omitempty"`
	ProjectToForkFrom       *string               `groups:"create" json:"project_to_fork_from,omitempty"`
	PublicAccess            *dtoPublicAccess      `groups:"create,update" json:"public_access,omitempty"`
	RecoveryTargetTime      *string               `groups:"create" json:"recovery_target_time,omitempty"`
	ServiceToForkFrom       *string               `groups:"create" json:"service_to_fork_from,omitempty"`
	SharedBuffersPercentage *float64              `groups:"create,update" json:"shared_buffers_percentage,omitempty"`
	StaticIps               *bool                 `groups:"create,update" json:"static_ips,omitempty"`
	SynchronousReplication  *string               `groups:"create,update" json:"synchronous_replication,omitempty"`
	Timescaledb             *dtoTimescaledb       `groups:"create,update" json:"timescaledb,omitempty"`
	Variant                 *string               `groups:"create,update" json:"variant,omitempty"`
	WorkMem                 *int64                `groups:"create,update" json:"work_mem,omitempty"`
}

// expandUserConfig expands tf object into dto object
func expandUserConfig(ctx context.Context, diags diag.Diagnostics, o *tfoUserConfig) *dtoUserConfig {
	additionalBackupRegionsVar := schemautil.ExpandSet[string](ctx, diags, o.AdditionalBackupRegions)
	if diags.HasError() {
		return nil
	}
	ipFilterVar := schemautil.ExpandSetNested(ctx, diags, expandIpFilter, o.IpFilter)
	if diags.HasError() {
		return nil
	}
	migrationVar := schemautil.ExpandSetBlockNested(ctx, diags, expandMigration, o.Migration)
	if diags.HasError() {
		return nil
	}
	pgVar := schemautil.ExpandSetBlockNested(ctx, diags, expandPg, o.Pg)
	if diags.HasError() {
		return nil
	}
	pgbouncerVar := schemautil.ExpandSetBlockNested(ctx, diags, expandPgbouncer, o.Pgbouncer)
	if diags.HasError() {
		return nil
	}
	pglookoutVar := schemautil.ExpandSetBlockNested(ctx, diags, expandPglookout, o.Pglookout)
	if diags.HasError() {
		return nil
	}
	privateAccessVar := schemautil.ExpandSetBlockNested(ctx, diags, expandPrivateAccess, o.PrivateAccess)
	if diags.HasError() {
		return nil
	}
	privatelinkAccessVar := schemautil.ExpandSetBlockNested(ctx, diags, expandPrivatelinkAccess, o.PrivatelinkAccess)
	if diags.HasError() {
		return nil
	}
	publicAccessVar := schemautil.ExpandSetBlockNested(ctx, diags, expandPublicAccess, o.PublicAccess)
	if diags.HasError() {
		return nil
	}
	timescaledbVar := schemautil.ExpandSetBlockNested(ctx, diags, expandTimescaledb, o.Timescaledb)
	if diags.HasError() {
		return nil
	}
	return &dtoUserConfig{
		AdditionalBackupRegions: additionalBackupRegionsVar,
		AdminPassword:           schemautil.ValueStringPointer(o.AdminPassword),
		AdminUsername:           schemautil.ValueStringPointer(o.AdminUsername),
		BackupHour:              schemautil.ValueInt64Pointer(o.BackupHour),
		BackupMinute:            schemautil.ValueInt64Pointer(o.BackupMinute),
		EnableIpv6:              schemautil.ValueBoolPointer(o.EnableIpv6),
		IpFilter:                ipFilterVar,
		Migration:               migrationVar,
		Pg:                      pgVar,
		PgReadReplica:           schemautil.ValueBoolPointer(o.PgReadReplica),
		PgServiceToForkFrom:     schemautil.ValueStringPointer(o.PgServiceToForkFrom),
		PgStatMonitorEnable:     schemautil.ValueBoolPointer(o.PgStatMonitorEnable),
		PgVersion:               schemautil.ValueStringPointer(o.PgVersion),
		Pgbouncer:               pgbouncerVar,
		Pglookout:               pglookoutVar,
		PrivateAccess:           privateAccessVar,
		PrivatelinkAccess:       privatelinkAccessVar,
		ProjectToForkFrom:       schemautil.ValueStringPointer(o.ProjectToForkFrom),
		PublicAccess:            publicAccessVar,
		RecoveryTargetTime:      schemautil.ValueStringPointer(o.RecoveryTargetTime),
		ServiceToForkFrom:       schemautil.ValueStringPointer(o.ServiceToForkFrom),
		SharedBuffersPercentage: schemautil.ValueFloat64Pointer(o.SharedBuffersPercentage),
		StaticIps:               schemautil.ValueBoolPointer(o.StaticIps),
		SynchronousReplication:  schemautil.ValueStringPointer(o.SynchronousReplication),
		Timescaledb:             timescaledbVar,
		Variant:                 schemautil.ValueStringPointer(o.Variant),
		WorkMem:                 schemautil.ValueInt64Pointer(o.WorkMem),
	}
}

// flattenUserConfig flattens dto object into tf object
func flattenUserConfig(ctx context.Context, diags diag.Diagnostics, o *dtoUserConfig) *tfoUserConfig {
	additionalBackupRegionsVar, d := types.SetValueFrom(ctx, types.StringType, o.AdditionalBackupRegions)
	diags.Append(d...)
	if diags.HasError() {
		return nil
	}
	ipFilterVar := schemautil.FlattenSetNested(ctx, diags, flattenIpFilter, o.IpFilter, ipFilterAttrs)
	if diags.HasError() {
		return nil
	}
	migrationVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenMigration, o.Migration, migrationAttrs)
	if diags.HasError() {
		return nil
	}
	pgVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenPg, o.Pg, pgAttrs)
	if diags.HasError() {
		return nil
	}
	pgbouncerVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenPgbouncer, o.Pgbouncer, pgbouncerAttrs)
	if diags.HasError() {
		return nil
	}
	pglookoutVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenPglookout, o.Pglookout, pglookoutAttrs)
	if diags.HasError() {
		return nil
	}
	privateAccessVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenPrivateAccess, o.PrivateAccess, privateAccessAttrs)
	if diags.HasError() {
		return nil
	}
	privatelinkAccessVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenPrivatelinkAccess, o.PrivatelinkAccess, privatelinkAccessAttrs)
	if diags.HasError() {
		return nil
	}
	publicAccessVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenPublicAccess, o.PublicAccess, publicAccessAttrs)
	if diags.HasError() {
		return nil
	}
	timescaledbVar := schemautil.FlattenSetBlockNested(ctx, diags, flattenTimescaledb, o.Timescaledb, timescaledbAttrs)
	if diags.HasError() {
		return nil
	}
	return &tfoUserConfig{
		AdditionalBackupRegions: additionalBackupRegionsVar,
		AdminPassword:           types.StringPointerValue(o.AdminPassword),
		AdminUsername:           types.StringPointerValue(o.AdminUsername),
		BackupHour:              types.Int64PointerValue(o.BackupHour),
		BackupMinute:            types.Int64PointerValue(o.BackupMinute),
		EnableIpv6:              types.BoolPointerValue(o.EnableIpv6),
		IpFilter:                ipFilterVar,
		Migration:               migrationVar,
		Pg:                      pgVar,
		PgReadReplica:           types.BoolPointerValue(o.PgReadReplica),
		PgServiceToForkFrom:     types.StringPointerValue(o.PgServiceToForkFrom),
		PgStatMonitorEnable:     types.BoolPointerValue(o.PgStatMonitorEnable),
		PgVersion:               types.StringPointerValue(o.PgVersion),
		Pgbouncer:               pgbouncerVar,
		Pglookout:               pglookoutVar,
		PrivateAccess:           privateAccessVar,
		PrivatelinkAccess:       privatelinkAccessVar,
		ProjectToForkFrom:       types.StringPointerValue(o.ProjectToForkFrom),
		PublicAccess:            publicAccessVar,
		RecoveryTargetTime:      types.StringPointerValue(o.RecoveryTargetTime),
		ServiceToForkFrom:       types.StringPointerValue(o.ServiceToForkFrom),
		SharedBuffersPercentage: types.Float64PointerValue(o.SharedBuffersPercentage),
		StaticIps:               types.BoolPointerValue(o.StaticIps),
		SynchronousReplication:  types.StringPointerValue(o.SynchronousReplication),
		Timescaledb:             timescaledbVar,
		Variant:                 types.StringPointerValue(o.Variant),
		WorkMem:                 types.Int64PointerValue(o.WorkMem),
	}
}

var userConfigAttrs = map[string]attr.Type{
	"additional_backup_regions": types.SetType{ElemType: types.StringType},
	"admin_password":            types.StringType,
	"admin_username":            types.StringType,
	"backup_hour":               types.Int64Type,
	"backup_minute":             types.Int64Type,
	"enable_ipv6":               types.BoolType,
	"ip_filter":                 types.SetType{ElemType: types.ObjectType{AttrTypes: ipFilterAttrs}},
	"migration":                 types.SetType{ElemType: types.ObjectType{AttrTypes: migrationAttrs}},
	"pg":                        types.SetType{ElemType: types.ObjectType{AttrTypes: pgAttrs}},
	"pg_read_replica":           types.BoolType,
	"pg_service_to_fork_from":   types.StringType,
	"pg_stat_monitor_enable":    types.BoolType,
	"pg_version":                types.StringType,
	"pgbouncer":                 types.SetType{ElemType: types.ObjectType{AttrTypes: pgbouncerAttrs}},
	"pglookout":                 types.SetType{ElemType: types.ObjectType{AttrTypes: pglookoutAttrs}},
	"private_access":            types.SetType{ElemType: types.ObjectType{AttrTypes: privateAccessAttrs}},
	"privatelink_access":        types.SetType{ElemType: types.ObjectType{AttrTypes: privatelinkAccessAttrs}},
	"project_to_fork_from":      types.StringType,
	"public_access":             types.SetType{ElemType: types.ObjectType{AttrTypes: publicAccessAttrs}},
	"recovery_target_time":      types.StringType,
	"service_to_fork_from":      types.StringType,
	"shared_buffers_percentage": types.Float64Type,
	"static_ips":                types.BoolType,
	"synchronous_replication":   types.StringType,
	"timescaledb":               types.SetType{ElemType: types.ObjectType{AttrTypes: timescaledbAttrs}},
	"variant":                   types.StringType,
	"work_mem":                  types.Int64Type,
}

// tfoIpFilter CIDR address block, either as a string, or in a dict with an optional description field
type tfoIpFilter struct {
	Description types.String `tfsdk:"description"`
	Network     types.String `tfsdk:"network"`
}

// dtoIpFilter request/response object
type dtoIpFilter struct {
	Description *string `groups:"create,update" json:"description,omitempty"`
	Network     string  `groups:"create,update" json:"network"`
}

func (d *dtoIpFilter) UnmarshalJSON(data []byte) error {
	var s string
	err := json.Unmarshal(data, &s)
	if err == nil {
		d.Network = s
		return nil
	}

	type obj dtoIpFilter
	o := &struct {
		Description *string `groups:"create,update" json:"description,omitempty"`
		Network     string  `groups:"create,update" json:"network"`
	}{}
	err = json.Unmarshal(data, o)
	if err != nil {
		return err
	}

	d.Description = o.Description
	d.Network = o.Network
	return nil
}

// expandIpFilter expands tf object into dto object
func expandIpFilter(ctx context.Context, diags diag.Diagnostics, o *tfoIpFilter) *dtoIpFilter {
	return &dtoIpFilter{
		Description: schemautil.ValueStringPointer(o.Description),
		Network:     o.Network.ValueString(),
	}
}

// flattenIpFilter flattens dto object into tf object
func flattenIpFilter(ctx context.Context, diags diag.Diagnostics, o *dtoIpFilter) *tfoIpFilter {
	return &tfoIpFilter{
		Description: types.StringPointerValue(o.Description),
		Network:     types.StringValue(o.Network),
	}
}

var ipFilterAttrs = map[string]attr.Type{
	"description": types.StringType,
	"network":     types.StringType,
}

// tfoMigration Migrate data from existing server
type tfoMigration struct {
	Dbname    types.String `tfsdk:"dbname"`
	Host      types.String `tfsdk:"host"`
	IgnoreDbs types.String `tfsdk:"ignore_dbs"`
	Method    types.String `tfsdk:"method"`
	Password  types.String `tfsdk:"password"`
	Port      types.Int64  `tfsdk:"port"`
	Ssl       types.Bool   `tfsdk:"ssl"`
	Username  types.String `tfsdk:"username"`
}

// dtoMigration request/response object
type dtoMigration struct {
	Dbname    *string `groups:"create,update" json:"dbname,omitempty"`
	Host      string  `groups:"create,update" json:"host"`
	IgnoreDbs *string `groups:"create,update" json:"ignore_dbs,omitempty"`
	Method    *string `groups:"create,update" json:"method,omitempty"`
	Password  *string `groups:"create,update" json:"password,omitempty"`
	Port      int64   `groups:"create,update" json:"port"`
	Ssl       *bool   `groups:"create,update" json:"ssl,omitempty"`
	Username  *string `groups:"create,update" json:"username,omitempty"`
}

// expandMigration expands tf object into dto object
func expandMigration(ctx context.Context, diags diag.Diagnostics, o *tfoMigration) *dtoMigration {
	return &dtoMigration{
		Dbname:    schemautil.ValueStringPointer(o.Dbname),
		Host:      o.Host.ValueString(),
		IgnoreDbs: schemautil.ValueStringPointer(o.IgnoreDbs),
		Method:    schemautil.ValueStringPointer(o.Method),
		Password:  schemautil.ValueStringPointer(o.Password),
		Port:      o.Port.ValueInt64(),
		Ssl:       schemautil.ValueBoolPointer(o.Ssl),
		Username:  schemautil.ValueStringPointer(o.Username),
	}
}

// flattenMigration flattens dto object into tf object
func flattenMigration(ctx context.Context, diags diag.Diagnostics, o *dtoMigration) *tfoMigration {
	return &tfoMigration{
		Dbname:    types.StringPointerValue(o.Dbname),
		Host:      types.StringValue(o.Host),
		IgnoreDbs: types.StringPointerValue(o.IgnoreDbs),
		Method:    types.StringPointerValue(o.Method),
		Password:  types.StringPointerValue(o.Password),
		Port:      types.Int64Value(o.Port),
		Ssl:       types.BoolPointerValue(o.Ssl),
		Username:  types.StringPointerValue(o.Username),
	}
}

var migrationAttrs = map[string]attr.Type{
	"dbname":     types.StringType,
	"host":       types.StringType,
	"ignore_dbs": types.StringType,
	"method":     types.StringType,
	"password":   types.StringType,
	"port":       types.Int64Type,
	"ssl":        types.BoolType,
	"username":   types.StringType,
}

// tfoPg postgresql.conf configuration values
type tfoPg struct {
	AutovacuumAnalyzeScaleFactor     types.Float64 `tfsdk:"autovacuum_analyze_scale_factor"`
	AutovacuumAnalyzeThreshold       types.Int64   `tfsdk:"autovacuum_analyze_threshold"`
	AutovacuumFreezeMaxAge           types.Int64   `tfsdk:"autovacuum_freeze_max_age"`
	AutovacuumMaxWorkers             types.Int64   `tfsdk:"autovacuum_max_workers"`
	AutovacuumNaptime                types.Int64   `tfsdk:"autovacuum_naptime"`
	AutovacuumVacuumCostDelay        types.Int64   `tfsdk:"autovacuum_vacuum_cost_delay"`
	AutovacuumVacuumCostLimit        types.Int64   `tfsdk:"autovacuum_vacuum_cost_limit"`
	AutovacuumVacuumScaleFactor      types.Float64 `tfsdk:"autovacuum_vacuum_scale_factor"`
	AutovacuumVacuumThreshold        types.Int64   `tfsdk:"autovacuum_vacuum_threshold"`
	BgwriterDelay                    types.Int64   `tfsdk:"bgwriter_delay"`
	BgwriterFlushAfter               types.Int64   `tfsdk:"bgwriter_flush_after"`
	BgwriterLruMaxpages              types.Int64   `tfsdk:"bgwriter_lru_maxpages"`
	BgwriterLruMultiplier            types.Float64 `tfsdk:"bgwriter_lru_multiplier"`
	DeadlockTimeout                  types.Int64   `tfsdk:"deadlock_timeout"`
	DefaultToastCompression          types.String  `tfsdk:"default_toast_compression"`
	IdleInTransactionSessionTimeout  types.Int64   `tfsdk:"idle_in_transaction_session_timeout"`
	Jit                              types.Bool    `tfsdk:"jit"`
	LogAutovacuumMinDuration         types.Int64   `tfsdk:"log_autovacuum_min_duration"`
	LogErrorVerbosity                types.String  `tfsdk:"log_error_verbosity"`
	LogLinePrefix                    types.String  `tfsdk:"log_line_prefix"`
	LogMinDurationStatement          types.Int64   `tfsdk:"log_min_duration_statement"`
	LogTempFiles                     types.Int64   `tfsdk:"log_temp_files"`
	MaxFilesPerProcess               types.Int64   `tfsdk:"max_files_per_process"`
	MaxLocksPerTransaction           types.Int64   `tfsdk:"max_locks_per_transaction"`
	MaxLogicalReplicationWorkers     types.Int64   `tfsdk:"max_logical_replication_workers"`
	MaxParallelWorkers               types.Int64   `tfsdk:"max_parallel_workers"`
	MaxParallelWorkersPerGather      types.Int64   `tfsdk:"max_parallel_workers_per_gather"`
	MaxPredLocksPerTransaction       types.Int64   `tfsdk:"max_pred_locks_per_transaction"`
	MaxPreparedTransactions          types.Int64   `tfsdk:"max_prepared_transactions"`
	MaxReplicationSlots              types.Int64   `tfsdk:"max_replication_slots"`
	MaxSlotWalKeepSize               types.Int64   `tfsdk:"max_slot_wal_keep_size"`
	MaxStackDepth                    types.Int64   `tfsdk:"max_stack_depth"`
	MaxStandbyArchiveDelay           types.Int64   `tfsdk:"max_standby_archive_delay"`
	MaxStandbyStreamingDelay         types.Int64   `tfsdk:"max_standby_streaming_delay"`
	MaxWalSenders                    types.Int64   `tfsdk:"max_wal_senders"`
	MaxWorkerProcesses               types.Int64   `tfsdk:"max_worker_processes"`
	PgPartmanBgwInterval             types.Int64   `tfsdk:"pg_partman_bgw__interval"`
	PgPartmanBgwRole                 types.String  `tfsdk:"pg_partman_bgw__role"`
	PgStatMonitorPgsmEnableQueryPlan types.Bool    `tfsdk:"pg_stat_monitor__pgsm_enable_query_plan"`
	PgStatMonitorPgsmMaxBuckets      types.Int64   `tfsdk:"pg_stat_monitor__pgsm_max_buckets"`
	PgStatStatementsTrack            types.String  `tfsdk:"pg_stat_statements__track"`
	TempFileLimit                    types.Int64   `tfsdk:"temp_file_limit"`
	Timezone                         types.String  `tfsdk:"timezone"`
	TrackActivityQuerySize           types.Int64   `tfsdk:"track_activity_query_size"`
	TrackCommitTimestamp             types.String  `tfsdk:"track_commit_timestamp"`
	TrackFunctions                   types.String  `tfsdk:"track_functions"`
	TrackIoTiming                    types.String  `tfsdk:"track_io_timing"`
	WalSenderTimeout                 types.Int64   `tfsdk:"wal_sender_timeout"`
	WalWriterDelay                   types.Int64   `tfsdk:"wal_writer_delay"`
}

// dtoPg request/response object
type dtoPg struct {
	AutovacuumAnalyzeScaleFactor     *float64 `groups:"create,update" json:"autovacuum_analyze_scale_factor,omitempty"`
	AutovacuumAnalyzeThreshold       *int64   `groups:"create,update" json:"autovacuum_analyze_threshold,omitempty"`
	AutovacuumFreezeMaxAge           *int64   `groups:"create,update" json:"autovacuum_freeze_max_age,omitempty"`
	AutovacuumMaxWorkers             *int64   `groups:"create,update" json:"autovacuum_max_workers,omitempty"`
	AutovacuumNaptime                *int64   `groups:"create,update" json:"autovacuum_naptime,omitempty"`
	AutovacuumVacuumCostDelay        *int64   `groups:"create,update" json:"autovacuum_vacuum_cost_delay,omitempty"`
	AutovacuumVacuumCostLimit        *int64   `groups:"create,update" json:"autovacuum_vacuum_cost_limit,omitempty"`
	AutovacuumVacuumScaleFactor      *float64 `groups:"create,update" json:"autovacuum_vacuum_scale_factor,omitempty"`
	AutovacuumVacuumThreshold        *int64   `groups:"create,update" json:"autovacuum_vacuum_threshold,omitempty"`
	BgwriterDelay                    *int64   `groups:"create,update" json:"bgwriter_delay,omitempty"`
	BgwriterFlushAfter               *int64   `groups:"create,update" json:"bgwriter_flush_after,omitempty"`
	BgwriterLruMaxpages              *int64   `groups:"create,update" json:"bgwriter_lru_maxpages,omitempty"`
	BgwriterLruMultiplier            *float64 `groups:"create,update" json:"bgwriter_lru_multiplier,omitempty"`
	DeadlockTimeout                  *int64   `groups:"create,update" json:"deadlock_timeout,omitempty"`
	DefaultToastCompression          *string  `groups:"create,update" json:"default_toast_compression,omitempty"`
	IdleInTransactionSessionTimeout  *int64   `groups:"create,update" json:"idle_in_transaction_session_timeout,omitempty"`
	Jit                              *bool    `groups:"create,update" json:"jit,omitempty"`
	LogAutovacuumMinDuration         *int64   `groups:"create,update" json:"log_autovacuum_min_duration,omitempty"`
	LogErrorVerbosity                *string  `groups:"create,update" json:"log_error_verbosity,omitempty"`
	LogLinePrefix                    *string  `groups:"create,update" json:"log_line_prefix,omitempty"`
	LogMinDurationStatement          *int64   `groups:"create,update" json:"log_min_duration_statement,omitempty"`
	LogTempFiles                     *int64   `groups:"create,update" json:"log_temp_files,omitempty"`
	MaxFilesPerProcess               *int64   `groups:"create,update" json:"max_files_per_process,omitempty"`
	MaxLocksPerTransaction           *int64   `groups:"create,update" json:"max_locks_per_transaction,omitempty"`
	MaxLogicalReplicationWorkers     *int64   `groups:"create,update" json:"max_logical_replication_workers,omitempty"`
	MaxParallelWorkers               *int64   `groups:"create,update" json:"max_parallel_workers,omitempty"`
	MaxParallelWorkersPerGather      *int64   `groups:"create,update" json:"max_parallel_workers_per_gather,omitempty"`
	MaxPredLocksPerTransaction       *int64   `groups:"create,update" json:"max_pred_locks_per_transaction,omitempty"`
	MaxPreparedTransactions          *int64   `groups:"create,update" json:"max_prepared_transactions,omitempty"`
	MaxReplicationSlots              *int64   `groups:"create,update" json:"max_replication_slots,omitempty"`
	MaxSlotWalKeepSize               *int64   `groups:"create,update" json:"max_slot_wal_keep_size,omitempty"`
	MaxStackDepth                    *int64   `groups:"create,update" json:"max_stack_depth,omitempty"`
	MaxStandbyArchiveDelay           *int64   `groups:"create,update" json:"max_standby_archive_delay,omitempty"`
	MaxStandbyStreamingDelay         *int64   `groups:"create,update" json:"max_standby_streaming_delay,omitempty"`
	MaxWalSenders                    *int64   `groups:"create,update" json:"max_wal_senders,omitempty"`
	MaxWorkerProcesses               *int64   `groups:"create,update" json:"max_worker_processes,omitempty"`
	PgPartmanBgwInterval             *int64   `groups:"create,update" json:"pg_partman_bgw.interval,omitempty"`
	PgPartmanBgwRole                 *string  `groups:"create,update" json:"pg_partman_bgw.role,omitempty"`
	PgStatMonitorPgsmEnableQueryPlan *bool    `groups:"create,update" json:"pg_stat_monitor.pgsm_enable_query_plan,omitempty"`
	PgStatMonitorPgsmMaxBuckets      *int64   `groups:"create,update" json:"pg_stat_monitor.pgsm_max_buckets,omitempty"`
	PgStatStatementsTrack            *string  `groups:"create,update" json:"pg_stat_statements.track,omitempty"`
	TempFileLimit                    *int64   `groups:"create,update" json:"temp_file_limit,omitempty"`
	Timezone                         *string  `groups:"create,update" json:"timezone,omitempty"`
	TrackActivityQuerySize           *int64   `groups:"create,update" json:"track_activity_query_size,omitempty"`
	TrackCommitTimestamp             *string  `groups:"create,update" json:"track_commit_timestamp,omitempty"`
	TrackFunctions                   *string  `groups:"create,update" json:"track_functions,omitempty"`
	TrackIoTiming                    *string  `groups:"create,update" json:"track_io_timing,omitempty"`
	WalSenderTimeout                 *int64   `groups:"create,update" json:"wal_sender_timeout,omitempty"`
	WalWriterDelay                   *int64   `groups:"create,update" json:"wal_writer_delay,omitempty"`
}

// expandPg expands tf object into dto object
func expandPg(ctx context.Context, diags diag.Diagnostics, o *tfoPg) *dtoPg {
	return &dtoPg{
		AutovacuumAnalyzeScaleFactor:     schemautil.ValueFloat64Pointer(o.AutovacuumAnalyzeScaleFactor),
		AutovacuumAnalyzeThreshold:       schemautil.ValueInt64Pointer(o.AutovacuumAnalyzeThreshold),
		AutovacuumFreezeMaxAge:           schemautil.ValueInt64Pointer(o.AutovacuumFreezeMaxAge),
		AutovacuumMaxWorkers:             schemautil.ValueInt64Pointer(o.AutovacuumMaxWorkers),
		AutovacuumNaptime:                schemautil.ValueInt64Pointer(o.AutovacuumNaptime),
		AutovacuumVacuumCostDelay:        schemautil.ValueInt64Pointer(o.AutovacuumVacuumCostDelay),
		AutovacuumVacuumCostLimit:        schemautil.ValueInt64Pointer(o.AutovacuumVacuumCostLimit),
		AutovacuumVacuumScaleFactor:      schemautil.ValueFloat64Pointer(o.AutovacuumVacuumScaleFactor),
		AutovacuumVacuumThreshold:        schemautil.ValueInt64Pointer(o.AutovacuumVacuumThreshold),
		BgwriterDelay:                    schemautil.ValueInt64Pointer(o.BgwriterDelay),
		BgwriterFlushAfter:               schemautil.ValueInt64Pointer(o.BgwriterFlushAfter),
		BgwriterLruMaxpages:              schemautil.ValueInt64Pointer(o.BgwriterLruMaxpages),
		BgwriterLruMultiplier:            schemautil.ValueFloat64Pointer(o.BgwriterLruMultiplier),
		DeadlockTimeout:                  schemautil.ValueInt64Pointer(o.DeadlockTimeout),
		DefaultToastCompression:          schemautil.ValueStringPointer(o.DefaultToastCompression),
		IdleInTransactionSessionTimeout:  schemautil.ValueInt64Pointer(o.IdleInTransactionSessionTimeout),
		Jit:                              schemautil.ValueBoolPointer(o.Jit),
		LogAutovacuumMinDuration:         schemautil.ValueInt64Pointer(o.LogAutovacuumMinDuration),
		LogErrorVerbosity:                schemautil.ValueStringPointer(o.LogErrorVerbosity),
		LogLinePrefix:                    schemautil.ValueStringPointer(o.LogLinePrefix),
		LogMinDurationStatement:          schemautil.ValueInt64Pointer(o.LogMinDurationStatement),
		LogTempFiles:                     schemautil.ValueInt64Pointer(o.LogTempFiles),
		MaxFilesPerProcess:               schemautil.ValueInt64Pointer(o.MaxFilesPerProcess),
		MaxLocksPerTransaction:           schemautil.ValueInt64Pointer(o.MaxLocksPerTransaction),
		MaxLogicalReplicationWorkers:     schemautil.ValueInt64Pointer(o.MaxLogicalReplicationWorkers),
		MaxParallelWorkers:               schemautil.ValueInt64Pointer(o.MaxParallelWorkers),
		MaxParallelWorkersPerGather:      schemautil.ValueInt64Pointer(o.MaxParallelWorkersPerGather),
		MaxPredLocksPerTransaction:       schemautil.ValueInt64Pointer(o.MaxPredLocksPerTransaction),
		MaxPreparedTransactions:          schemautil.ValueInt64Pointer(o.MaxPreparedTransactions),
		MaxReplicationSlots:              schemautil.ValueInt64Pointer(o.MaxReplicationSlots),
		MaxSlotWalKeepSize:               schemautil.ValueInt64Pointer(o.MaxSlotWalKeepSize),
		MaxStackDepth:                    schemautil.ValueInt64Pointer(o.MaxStackDepth),
		MaxStandbyArchiveDelay:           schemautil.ValueInt64Pointer(o.MaxStandbyArchiveDelay),
		MaxStandbyStreamingDelay:         schemautil.ValueInt64Pointer(o.MaxStandbyStreamingDelay),
		MaxWalSenders:                    schemautil.ValueInt64Pointer(o.MaxWalSenders),
		MaxWorkerProcesses:               schemautil.ValueInt64Pointer(o.MaxWorkerProcesses),
		PgPartmanBgwInterval:             schemautil.ValueInt64Pointer(o.PgPartmanBgwInterval),
		PgPartmanBgwRole:                 schemautil.ValueStringPointer(o.PgPartmanBgwRole),
		PgStatMonitorPgsmEnableQueryPlan: schemautil.ValueBoolPointer(o.PgStatMonitorPgsmEnableQueryPlan),
		PgStatMonitorPgsmMaxBuckets:      schemautil.ValueInt64Pointer(o.PgStatMonitorPgsmMaxBuckets),
		PgStatStatementsTrack:            schemautil.ValueStringPointer(o.PgStatStatementsTrack),
		TempFileLimit:                    schemautil.ValueInt64Pointer(o.TempFileLimit),
		Timezone:                         schemautil.ValueStringPointer(o.Timezone),
		TrackActivityQuerySize:           schemautil.ValueInt64Pointer(o.TrackActivityQuerySize),
		TrackCommitTimestamp:             schemautil.ValueStringPointer(o.TrackCommitTimestamp),
		TrackFunctions:                   schemautil.ValueStringPointer(o.TrackFunctions),
		TrackIoTiming:                    schemautil.ValueStringPointer(o.TrackIoTiming),
		WalSenderTimeout:                 schemautil.ValueInt64Pointer(o.WalSenderTimeout),
		WalWriterDelay:                   schemautil.ValueInt64Pointer(o.WalWriterDelay),
	}
}

// flattenPg flattens dto object into tf object
func flattenPg(ctx context.Context, diags diag.Diagnostics, o *dtoPg) *tfoPg {
	return &tfoPg{
		AutovacuumAnalyzeScaleFactor:     types.Float64PointerValue(o.AutovacuumAnalyzeScaleFactor),
		AutovacuumAnalyzeThreshold:       types.Int64PointerValue(o.AutovacuumAnalyzeThreshold),
		AutovacuumFreezeMaxAge:           types.Int64PointerValue(o.AutovacuumFreezeMaxAge),
		AutovacuumMaxWorkers:             types.Int64PointerValue(o.AutovacuumMaxWorkers),
		AutovacuumNaptime:                types.Int64PointerValue(o.AutovacuumNaptime),
		AutovacuumVacuumCostDelay:        types.Int64PointerValue(o.AutovacuumVacuumCostDelay),
		AutovacuumVacuumCostLimit:        types.Int64PointerValue(o.AutovacuumVacuumCostLimit),
		AutovacuumVacuumScaleFactor:      types.Float64PointerValue(o.AutovacuumVacuumScaleFactor),
		AutovacuumVacuumThreshold:        types.Int64PointerValue(o.AutovacuumVacuumThreshold),
		BgwriterDelay:                    types.Int64PointerValue(o.BgwriterDelay),
		BgwriterFlushAfter:               types.Int64PointerValue(o.BgwriterFlushAfter),
		BgwriterLruMaxpages:              types.Int64PointerValue(o.BgwriterLruMaxpages),
		BgwriterLruMultiplier:            types.Float64PointerValue(o.BgwriterLruMultiplier),
		DeadlockTimeout:                  types.Int64PointerValue(o.DeadlockTimeout),
		DefaultToastCompression:          types.StringPointerValue(o.DefaultToastCompression),
		IdleInTransactionSessionTimeout:  types.Int64PointerValue(o.IdleInTransactionSessionTimeout),
		Jit:                              types.BoolPointerValue(o.Jit),
		LogAutovacuumMinDuration:         types.Int64PointerValue(o.LogAutovacuumMinDuration),
		LogErrorVerbosity:                types.StringPointerValue(o.LogErrorVerbosity),
		LogLinePrefix:                    types.StringPointerValue(o.LogLinePrefix),
		LogMinDurationStatement:          types.Int64PointerValue(o.LogMinDurationStatement),
		LogTempFiles:                     types.Int64PointerValue(o.LogTempFiles),
		MaxFilesPerProcess:               types.Int64PointerValue(o.MaxFilesPerProcess),
		MaxLocksPerTransaction:           types.Int64PointerValue(o.MaxLocksPerTransaction),
		MaxLogicalReplicationWorkers:     types.Int64PointerValue(o.MaxLogicalReplicationWorkers),
		MaxParallelWorkers:               types.Int64PointerValue(o.MaxParallelWorkers),
		MaxParallelWorkersPerGather:      types.Int64PointerValue(o.MaxParallelWorkersPerGather),
		MaxPredLocksPerTransaction:       types.Int64PointerValue(o.MaxPredLocksPerTransaction),
		MaxPreparedTransactions:          types.Int64PointerValue(o.MaxPreparedTransactions),
		MaxReplicationSlots:              types.Int64PointerValue(o.MaxReplicationSlots),
		MaxSlotWalKeepSize:               types.Int64PointerValue(o.MaxSlotWalKeepSize),
		MaxStackDepth:                    types.Int64PointerValue(o.MaxStackDepth),
		MaxStandbyArchiveDelay:           types.Int64PointerValue(o.MaxStandbyArchiveDelay),
		MaxStandbyStreamingDelay:         types.Int64PointerValue(o.MaxStandbyStreamingDelay),
		MaxWalSenders:                    types.Int64PointerValue(o.MaxWalSenders),
		MaxWorkerProcesses:               types.Int64PointerValue(o.MaxWorkerProcesses),
		PgPartmanBgwInterval:             types.Int64PointerValue(o.PgPartmanBgwInterval),
		PgPartmanBgwRole:                 types.StringPointerValue(o.PgPartmanBgwRole),
		PgStatMonitorPgsmEnableQueryPlan: types.BoolPointerValue(o.PgStatMonitorPgsmEnableQueryPlan),
		PgStatMonitorPgsmMaxBuckets:      types.Int64PointerValue(o.PgStatMonitorPgsmMaxBuckets),
		PgStatStatementsTrack:            types.StringPointerValue(o.PgStatStatementsTrack),
		TempFileLimit:                    types.Int64PointerValue(o.TempFileLimit),
		Timezone:                         types.StringPointerValue(o.Timezone),
		TrackActivityQuerySize:           types.Int64PointerValue(o.TrackActivityQuerySize),
		TrackCommitTimestamp:             types.StringPointerValue(o.TrackCommitTimestamp),
		TrackFunctions:                   types.StringPointerValue(o.TrackFunctions),
		TrackIoTiming:                    types.StringPointerValue(o.TrackIoTiming),
		WalSenderTimeout:                 types.Int64PointerValue(o.WalSenderTimeout),
		WalWriterDelay:                   types.Int64PointerValue(o.WalWriterDelay),
	}
}

var pgAttrs = map[string]attr.Type{
	"autovacuum_analyze_scale_factor":         types.Float64Type,
	"autovacuum_analyze_threshold":            types.Int64Type,
	"autovacuum_freeze_max_age":               types.Int64Type,
	"autovacuum_max_workers":                  types.Int64Type,
	"autovacuum_naptime":                      types.Int64Type,
	"autovacuum_vacuum_cost_delay":            types.Int64Type,
	"autovacuum_vacuum_cost_limit":            types.Int64Type,
	"autovacuum_vacuum_scale_factor":          types.Float64Type,
	"autovacuum_vacuum_threshold":             types.Int64Type,
	"bgwriter_delay":                          types.Int64Type,
	"bgwriter_flush_after":                    types.Int64Type,
	"bgwriter_lru_maxpages":                   types.Int64Type,
	"bgwriter_lru_multiplier":                 types.Float64Type,
	"deadlock_timeout":                        types.Int64Type,
	"default_toast_compression":               types.StringType,
	"idle_in_transaction_session_timeout":     types.Int64Type,
	"jit":                                     types.BoolType,
	"log_autovacuum_min_duration":             types.Int64Type,
	"log_error_verbosity":                     types.StringType,
	"log_line_prefix":                         types.StringType,
	"log_min_duration_statement":              types.Int64Type,
	"log_temp_files":                          types.Int64Type,
	"max_files_per_process":                   types.Int64Type,
	"max_locks_per_transaction":               types.Int64Type,
	"max_logical_replication_workers":         types.Int64Type,
	"max_parallel_workers":                    types.Int64Type,
	"max_parallel_workers_per_gather":         types.Int64Type,
	"max_pred_locks_per_transaction":          types.Int64Type,
	"max_prepared_transactions":               types.Int64Type,
	"max_replication_slots":                   types.Int64Type,
	"max_slot_wal_keep_size":                  types.Int64Type,
	"max_stack_depth":                         types.Int64Type,
	"max_standby_archive_delay":               types.Int64Type,
	"max_standby_streaming_delay":             types.Int64Type,
	"max_wal_senders":                         types.Int64Type,
	"max_worker_processes":                    types.Int64Type,
	"pg_partman_bgw__interval":                types.Int64Type,
	"pg_partman_bgw__role":                    types.StringType,
	"pg_stat_monitor__pgsm_enable_query_plan": types.BoolType,
	"pg_stat_monitor__pgsm_max_buckets":       types.Int64Type,
	"pg_stat_statements__track":               types.StringType,
	"temp_file_limit":                         types.Int64Type,
	"timezone":                                types.StringType,
	"track_activity_query_size":               types.Int64Type,
	"track_commit_timestamp":                  types.StringType,
	"track_functions":                         types.StringType,
	"track_io_timing":                         types.StringType,
	"wal_sender_timeout":                      types.Int64Type,
	"wal_writer_delay":                        types.Int64Type,
}

// tfoPgbouncer PGBouncer connection pooling settings
type tfoPgbouncer struct {
	AutodbIdleTimeout       types.Int64  `tfsdk:"autodb_idle_timeout"`
	AutodbMaxDbConnections  types.Int64  `tfsdk:"autodb_max_db_connections"`
	AutodbPoolMode          types.String `tfsdk:"autodb_pool_mode"`
	AutodbPoolSize          types.Int64  `tfsdk:"autodb_pool_size"`
	IgnoreStartupParameters types.Set    `tfsdk:"ignore_startup_parameters"`
	MinPoolSize             types.Int64  `tfsdk:"min_pool_size"`
	ServerIdleTimeout       types.Int64  `tfsdk:"server_idle_timeout"`
	ServerLifetime          types.Int64  `tfsdk:"server_lifetime"`
	ServerResetQueryAlways  types.Bool   `tfsdk:"server_reset_query_always"`
}

// dtoPgbouncer request/response object
type dtoPgbouncer struct {
	AutodbIdleTimeout       *int64   `groups:"create,update" json:"autodb_idle_timeout,omitempty"`
	AutodbMaxDbConnections  *int64   `groups:"create,update" json:"autodb_max_db_connections,omitempty"`
	AutodbPoolMode          *string  `groups:"create,update" json:"autodb_pool_mode,omitempty"`
	AutodbPoolSize          *int64   `groups:"create,update" json:"autodb_pool_size,omitempty"`
	IgnoreStartupParameters []string `groups:"create,update" json:"ignore_startup_parameters,omitempty"`
	MinPoolSize             *int64   `groups:"create,update" json:"min_pool_size,omitempty"`
	ServerIdleTimeout       *int64   `groups:"create,update" json:"server_idle_timeout,omitempty"`
	ServerLifetime          *int64   `groups:"create,update" json:"server_lifetime,omitempty"`
	ServerResetQueryAlways  *bool    `groups:"create,update" json:"server_reset_query_always,omitempty"`
}

// expandPgbouncer expands tf object into dto object
func expandPgbouncer(ctx context.Context, diags diag.Diagnostics, o *tfoPgbouncer) *dtoPgbouncer {
	ignoreStartupParametersVar := schemautil.ExpandSet[string](ctx, diags, o.IgnoreStartupParameters)
	if diags.HasError() {
		return nil
	}
	return &dtoPgbouncer{
		AutodbIdleTimeout:       schemautil.ValueInt64Pointer(o.AutodbIdleTimeout),
		AutodbMaxDbConnections:  schemautil.ValueInt64Pointer(o.AutodbMaxDbConnections),
		AutodbPoolMode:          schemautil.ValueStringPointer(o.AutodbPoolMode),
		AutodbPoolSize:          schemautil.ValueInt64Pointer(o.AutodbPoolSize),
		IgnoreStartupParameters: ignoreStartupParametersVar,
		MinPoolSize:             schemautil.ValueInt64Pointer(o.MinPoolSize),
		ServerIdleTimeout:       schemautil.ValueInt64Pointer(o.ServerIdleTimeout),
		ServerLifetime:          schemautil.ValueInt64Pointer(o.ServerLifetime),
		ServerResetQueryAlways:  schemautil.ValueBoolPointer(o.ServerResetQueryAlways),
	}
}

// flattenPgbouncer flattens dto object into tf object
func flattenPgbouncer(ctx context.Context, diags diag.Diagnostics, o *dtoPgbouncer) *tfoPgbouncer {
	ignoreStartupParametersVar, d := types.SetValueFrom(ctx, types.StringType, o.IgnoreStartupParameters)
	diags.Append(d...)
	if diags.HasError() {
		return nil
	}
	return &tfoPgbouncer{
		AutodbIdleTimeout:       types.Int64PointerValue(o.AutodbIdleTimeout),
		AutodbMaxDbConnections:  types.Int64PointerValue(o.AutodbMaxDbConnections),
		AutodbPoolMode:          types.StringPointerValue(o.AutodbPoolMode),
		AutodbPoolSize:          types.Int64PointerValue(o.AutodbPoolSize),
		IgnoreStartupParameters: ignoreStartupParametersVar,
		MinPoolSize:             types.Int64PointerValue(o.MinPoolSize),
		ServerIdleTimeout:       types.Int64PointerValue(o.ServerIdleTimeout),
		ServerLifetime:          types.Int64PointerValue(o.ServerLifetime),
		ServerResetQueryAlways:  types.BoolPointerValue(o.ServerResetQueryAlways),
	}
}

var pgbouncerAttrs = map[string]attr.Type{
	"autodb_idle_timeout":       types.Int64Type,
	"autodb_max_db_connections": types.Int64Type,
	"autodb_pool_mode":          types.StringType,
	"autodb_pool_size":          types.Int64Type,
	"ignore_startup_parameters": types.SetType{ElemType: types.StringType},
	"min_pool_size":             types.Int64Type,
	"server_idle_timeout":       types.Int64Type,
	"server_lifetime":           types.Int64Type,
	"server_reset_query_always": types.BoolType,
}

// tfoPglookout PGLookout settings
type tfoPglookout struct {
	MaxFailoverReplicationTimeLag types.Int64 `tfsdk:"max_failover_replication_time_lag"`
}

// dtoPglookout request/response object
type dtoPglookout struct {
	MaxFailoverReplicationTimeLag *int64 `groups:"create,update" json:"max_failover_replication_time_lag,omitempty"`
}

// expandPglookout expands tf object into dto object
func expandPglookout(ctx context.Context, diags diag.Diagnostics, o *tfoPglookout) *dtoPglookout {
	return &dtoPglookout{MaxFailoverReplicationTimeLag: schemautil.ValueInt64Pointer(o.MaxFailoverReplicationTimeLag)}
}

// flattenPglookout flattens dto object into tf object
func flattenPglookout(ctx context.Context, diags diag.Diagnostics, o *dtoPglookout) *tfoPglookout {
	return &tfoPglookout{MaxFailoverReplicationTimeLag: types.Int64PointerValue(o.MaxFailoverReplicationTimeLag)}
}

var pglookoutAttrs = map[string]attr.Type{"max_failover_replication_time_lag": types.Int64Type}

// tfoPrivateAccess Allow access to selected service ports from private networks
type tfoPrivateAccess struct {
	Pg         types.Bool `tfsdk:"pg"`
	Pgbouncer  types.Bool `tfsdk:"pgbouncer"`
	Prometheus types.Bool `tfsdk:"prometheus"`
}

// dtoPrivateAccess request/response object
type dtoPrivateAccess struct {
	Pg         *bool `groups:"create,update" json:"pg,omitempty"`
	Pgbouncer  *bool `groups:"create,update" json:"pgbouncer,omitempty"`
	Prometheus *bool `groups:"create,update" json:"prometheus,omitempty"`
}

// expandPrivateAccess expands tf object into dto object
func expandPrivateAccess(ctx context.Context, diags diag.Diagnostics, o *tfoPrivateAccess) *dtoPrivateAccess {
	return &dtoPrivateAccess{
		Pg:         schemautil.ValueBoolPointer(o.Pg),
		Pgbouncer:  schemautil.ValueBoolPointer(o.Pgbouncer),
		Prometheus: schemautil.ValueBoolPointer(o.Prometheus),
	}
}

// flattenPrivateAccess flattens dto object into tf object
func flattenPrivateAccess(ctx context.Context, diags diag.Diagnostics, o *dtoPrivateAccess) *tfoPrivateAccess {
	return &tfoPrivateAccess{
		Pg:         types.BoolPointerValue(o.Pg),
		Pgbouncer:  types.BoolPointerValue(o.Pgbouncer),
		Prometheus: types.BoolPointerValue(o.Prometheus),
	}
}

var privateAccessAttrs = map[string]attr.Type{
	"pg":         types.BoolType,
	"pgbouncer":  types.BoolType,
	"prometheus": types.BoolType,
}

// tfoPrivatelinkAccess Allow access to selected service components through Privatelink
type tfoPrivatelinkAccess struct {
	Pg         types.Bool `tfsdk:"pg"`
	Pgbouncer  types.Bool `tfsdk:"pgbouncer"`
	Prometheus types.Bool `tfsdk:"prometheus"`
}

// dtoPrivatelinkAccess request/response object
type dtoPrivatelinkAccess struct {
	Pg         *bool `groups:"create,update" json:"pg,omitempty"`
	Pgbouncer  *bool `groups:"create,update" json:"pgbouncer,omitempty"`
	Prometheus *bool `groups:"create,update" json:"prometheus,omitempty"`
}

// expandPrivatelinkAccess expands tf object into dto object
func expandPrivatelinkAccess(ctx context.Context, diags diag.Diagnostics, o *tfoPrivatelinkAccess) *dtoPrivatelinkAccess {
	return &dtoPrivatelinkAccess{
		Pg:         schemautil.ValueBoolPointer(o.Pg),
		Pgbouncer:  schemautil.ValueBoolPointer(o.Pgbouncer),
		Prometheus: schemautil.ValueBoolPointer(o.Prometheus),
	}
}

// flattenPrivatelinkAccess flattens dto object into tf object
func flattenPrivatelinkAccess(ctx context.Context, diags diag.Diagnostics, o *dtoPrivatelinkAccess) *tfoPrivatelinkAccess {
	return &tfoPrivatelinkAccess{
		Pg:         types.BoolPointerValue(o.Pg),
		Pgbouncer:  types.BoolPointerValue(o.Pgbouncer),
		Prometheus: types.BoolPointerValue(o.Prometheus),
	}
}

var privatelinkAccessAttrs = map[string]attr.Type{
	"pg":         types.BoolType,
	"pgbouncer":  types.BoolType,
	"prometheus": types.BoolType,
}

// tfoPublicAccess Allow access to selected service ports from the public Internet
type tfoPublicAccess struct {
	Pg         types.Bool `tfsdk:"pg"`
	Pgbouncer  types.Bool `tfsdk:"pgbouncer"`
	Prometheus types.Bool `tfsdk:"prometheus"`
}

// dtoPublicAccess request/response object
type dtoPublicAccess struct {
	Pg         *bool `groups:"create,update" json:"pg,omitempty"`
	Pgbouncer  *bool `groups:"create,update" json:"pgbouncer,omitempty"`
	Prometheus *bool `groups:"create,update" json:"prometheus,omitempty"`
}

// expandPublicAccess expands tf object into dto object
func expandPublicAccess(ctx context.Context, diags diag.Diagnostics, o *tfoPublicAccess) *dtoPublicAccess {
	return &dtoPublicAccess{
		Pg:         schemautil.ValueBoolPointer(o.Pg),
		Pgbouncer:  schemautil.ValueBoolPointer(o.Pgbouncer),
		Prometheus: schemautil.ValueBoolPointer(o.Prometheus),
	}
}

// flattenPublicAccess flattens dto object into tf object
func flattenPublicAccess(ctx context.Context, diags diag.Diagnostics, o *dtoPublicAccess) *tfoPublicAccess {
	return &tfoPublicAccess{
		Pg:         types.BoolPointerValue(o.Pg),
		Pgbouncer:  types.BoolPointerValue(o.Pgbouncer),
		Prometheus: types.BoolPointerValue(o.Prometheus),
	}
}

var publicAccessAttrs = map[string]attr.Type{
	"pg":         types.BoolType,
	"pgbouncer":  types.BoolType,
	"prometheus": types.BoolType,
}

// tfoTimescaledb TimescaleDB extension configuration values
type tfoTimescaledb struct {
	MaxBackgroundWorkers types.Int64 `tfsdk:"max_background_workers"`
}

// dtoTimescaledb request/response object
type dtoTimescaledb struct {
	MaxBackgroundWorkers *int64 `groups:"create,update" json:"max_background_workers,omitempty"`
}

// expandTimescaledb expands tf object into dto object
func expandTimescaledb(ctx context.Context, diags diag.Diagnostics, o *tfoTimescaledb) *dtoTimescaledb {
	return &dtoTimescaledb{MaxBackgroundWorkers: schemautil.ValueInt64Pointer(o.MaxBackgroundWorkers)}
}

// flattenTimescaledb flattens dto object into tf object
func flattenTimescaledb(ctx context.Context, diags diag.Diagnostics, o *dtoTimescaledb) *tfoTimescaledb {
	return &tfoTimescaledb{MaxBackgroundWorkers: types.Int64PointerValue(o.MaxBackgroundWorkers)}
}

var timescaledbAttrs = map[string]attr.Type{"max_background_workers": types.Int64Type}

// Expand public function that converts tf object into dto
func Expand(ctx context.Context, diags diag.Diagnostics, set types.Set) *dtoUserConfig {
	return schemautil.ExpandSetBlockNested[tfoUserConfig, dtoUserConfig](ctx, diags, expandUserConfig, set)
}

// Flatten public function that converts dto into tf object
func Flatten(ctx context.Context, diags diag.Diagnostics, m map[string]any) types.Set {
	o := new(dtoUserConfig)
	err := schemautil.MapToDTO(m, o)
	if err != nil {
		diags.AddError("Failed to marshal map user config to dto", err.Error())
		return types.SetNull(types.ObjectType{AttrTypes: userConfigAttrs})
	}
	return schemautil.FlattenSetBlockNested[dtoUserConfig, tfoUserConfig](ctx, diags, flattenUserConfig, o, userConfigAttrs)
}
