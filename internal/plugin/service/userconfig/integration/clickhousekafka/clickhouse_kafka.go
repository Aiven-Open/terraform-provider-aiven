// Code generated by user config generator. DO NOT EDIT.

package clickhousekafka

import (
	"context"

	setvalidator "github.com/hashicorp/terraform-plugin-framework-validators/setvalidator"
	attr "github.com/hashicorp/terraform-plugin-framework/attr"
	datasource "github.com/hashicorp/terraform-plugin-framework/datasource/schema"
	diag "github.com/hashicorp/terraform-plugin-framework/diag"
	resource "github.com/hashicorp/terraform-plugin-framework/resource/schema"
	int64default "github.com/hashicorp/terraform-plugin-framework/resource/schema/int64default"
	stringdefault "github.com/hashicorp/terraform-plugin-framework/resource/schema/stringdefault"
	validator "github.com/hashicorp/terraform-plugin-framework/schema/validator"
	types "github.com/hashicorp/terraform-plugin-framework/types"

	schemautil "github.com/aiven/terraform-provider-aiven/internal/schemautil"
)

// NewResourceSchema returns resource schema
func NewResourceSchema() resource.SetNestedBlock {
	return resource.SetNestedBlock{
		Description: "Integration user config",
		NestedObject: resource.NestedBlockObject{Blocks: map[string]resource.Block{"tables": resource.SetNestedBlock{
			Description: "Tables to create",
			NestedObject: resource.NestedBlockObject{
				Attributes: map[string]resource.Attribute{
					"auto_offset_reset": resource.StringAttribute{
						Computed:    true,
						Default:     stringdefault.StaticString("earliest"),
						Description: "Action to take when there is no initial offset in offset store or the desired offset is out of range. The default value is `earliest`.",
						Optional:    true,
					},
					"data_format": resource.StringAttribute{
						Description: "Message data format. The default value is `JSONEachRow`.",
						Required:    true,
					},
					"date_time_input_format": resource.StringAttribute{
						Computed:    true,
						Default:     stringdefault.StaticString("basic"),
						Description: "Method to read DateTime from text input formats. The default value is `basic`.",
						Optional:    true,
					},
					"group_name": resource.StringAttribute{
						Description: "Kafka consumers group. The default value is `clickhouse`.",
						Required:    true,
					},
					"handle_error_mode": resource.StringAttribute{
						Computed:    true,
						Default:     stringdefault.StaticString("default"),
						Description: "How to handle errors for Kafka engine. The default value is `default`.",
						Optional:    true,
					},
					"max_block_size": resource.Int64Attribute{
						Computed:    true,
						Default:     int64default.StaticInt64(0),
						Description: "Number of row collected by poll(s) for flushing data from Kafka. The default value is `0`.",
						Optional:    true,
					},
					"max_rows_per_message": resource.Int64Attribute{
						Computed:    true,
						Default:     int64default.StaticInt64(1),
						Description: "The maximum number of rows produced in one kafka message for row-based formats. The default value is `1`.",
						Optional:    true,
					},
					"name": resource.StringAttribute{
						Description: "Name of the table.",
						Required:    true,
					},
					"num_consumers": resource.Int64Attribute{
						Computed:    true,
						Default:     int64default.StaticInt64(1),
						Description: "The number of consumers per table per replica. The default value is `1`.",
						Optional:    true,
					},
					"poll_max_batch_size": resource.Int64Attribute{
						Computed:    true,
						Default:     int64default.StaticInt64(0),
						Description: "Maximum amount of messages to be polled in a single Kafka poll. The default value is `0`.",
						Optional:    true,
					},
					"skip_broken_messages": resource.Int64Attribute{
						Computed:    true,
						Default:     int64default.StaticInt64(0),
						Description: "Skip at least this number of broken messages from Kafka topic per block. The default value is `0`.",
						Optional:    true,
					},
				},
				Blocks: map[string]resource.Block{
					"columns": resource.SetNestedBlock{
						Description: "Table columns",
						NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{
							"name": resource.StringAttribute{
								Description: "Column name.",
								Required:    true,
							},
							"type": resource.StringAttribute{
								Description: "Column type.",
								Required:    true,
							},
						}},
						Validators: []validator.Set{setvalidator.SizeAtMost(100)},
					},
					"topics": resource.SetNestedBlock{
						Description: "Kafka topics",
						NestedObject: resource.NestedBlockObject{Attributes: map[string]resource.Attribute{"name": resource.StringAttribute{
							Description: "Name of the topic.",
							Required:    true,
						}}},
						Validators: []validator.Set{setvalidator.SizeAtMost(100)},
					},
				},
			},
			Validators: []validator.Set{setvalidator.SizeAtMost(100)},
		}}},
		Validators: []validator.Set{setvalidator.SizeAtMost(1)},
	}
}

// NewDataSourceSchema returns datasource schema
func NewDataSourceSchema() datasource.SetNestedBlock {
	return datasource.SetNestedBlock{
		Description: "Integration user config",
		NestedObject: datasource.NestedBlockObject{Blocks: map[string]datasource.Block{"tables": datasource.SetNestedBlock{
			Description: "Tables to create",
			NestedObject: datasource.NestedBlockObject{
				Attributes: map[string]datasource.Attribute{
					"auto_offset_reset": datasource.StringAttribute{
						Computed:    true,
						Description: "Action to take when there is no initial offset in offset store or the desired offset is out of range. The default value is `earliest`.",
					},
					"data_format": datasource.StringAttribute{
						Computed:    true,
						Description: "Message data format. The default value is `JSONEachRow`.",
					},
					"date_time_input_format": datasource.StringAttribute{
						Computed:    true,
						Description: "Method to read DateTime from text input formats. The default value is `basic`.",
					},
					"group_name": datasource.StringAttribute{
						Computed:    true,
						Description: "Kafka consumers group. The default value is `clickhouse`.",
					},
					"handle_error_mode": datasource.StringAttribute{
						Computed:    true,
						Description: "How to handle errors for Kafka engine. The default value is `default`.",
					},
					"max_block_size": datasource.Int64Attribute{
						Computed:    true,
						Description: "Number of row collected by poll(s) for flushing data from Kafka. The default value is `0`.",
					},
					"max_rows_per_message": datasource.Int64Attribute{
						Computed:    true,
						Description: "The maximum number of rows produced in one kafka message for row-based formats. The default value is `1`.",
					},
					"name": datasource.StringAttribute{
						Computed:    true,
						Description: "Name of the table.",
					},
					"num_consumers": datasource.Int64Attribute{
						Computed:    true,
						Description: "The number of consumers per table per replica. The default value is `1`.",
					},
					"poll_max_batch_size": datasource.Int64Attribute{
						Computed:    true,
						Description: "Maximum amount of messages to be polled in a single Kafka poll. The default value is `0`.",
					},
					"skip_broken_messages": datasource.Int64Attribute{
						Computed:    true,
						Description: "Skip at least this number of broken messages from Kafka topic per block. The default value is `0`.",
					},
				},
				Blocks: map[string]datasource.Block{
					"columns": datasource.SetNestedBlock{
						Description: "Table columns",
						NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{
							"name": datasource.StringAttribute{
								Computed:    true,
								Description: "Column name.",
							},
							"type": datasource.StringAttribute{
								Computed:    true,
								Description: "Column type.",
							},
						}},
						Validators: []validator.Set{setvalidator.SizeAtMost(100)},
					},
					"topics": datasource.SetNestedBlock{
						Description: "Kafka topics",
						NestedObject: datasource.NestedBlockObject{Attributes: map[string]datasource.Attribute{"name": datasource.StringAttribute{
							Computed:    true,
							Description: "Name of the topic.",
						}}},
						Validators: []validator.Set{setvalidator.SizeAtMost(100)},
					},
				},
			},
			Validators: []validator.Set{setvalidator.SizeAtMost(100)},
		}}},
		Validators: []validator.Set{setvalidator.SizeAtMost(1)},
	}
}

// tfoUserConfig Integration user config
type tfoUserConfig struct {
	Tables types.Set `tfsdk:"tables"`
}

// dtoUserConfig request/response object
type dtoUserConfig struct {
	Tables []*dtoTables `groups:"create,update" json:"tables,omitempty"`
}

// expandUserConfig expands tf object into dto object
func expandUserConfig(ctx context.Context, diags *diag.Diagnostics, o *tfoUserConfig) *dtoUserConfig {
	tablesVar := schemautil.ExpandSetNested[tfoTables, dtoTables](ctx, diags, expandTables, o.Tables)
	if diags.HasError() {
		return nil
	}
	return &dtoUserConfig{Tables: tablesVar}
}

// flattenUserConfig flattens dto object into tf object
func flattenUserConfig(ctx context.Context, diags *diag.Diagnostics, o *dtoUserConfig) *tfoUserConfig {
	tablesVar := schemautil.FlattenSetNested[dtoTables, tfoTables](ctx, diags, flattenTables, tablesAttrs, o.Tables)
	if diags.HasError() {
		return nil
	}
	return &tfoUserConfig{Tables: tablesVar}
}

var userConfigAttrs = map[string]attr.Type{"tables": types.SetType{ElemType: types.ObjectType{AttrTypes: tablesAttrs}}}

// tfoTables Table to create
type tfoTables struct {
	AutoOffsetReset     types.String `tfsdk:"auto_offset_reset"`
	Columns             types.Set    `tfsdk:"columns"`
	DataFormat          types.String `tfsdk:"data_format"`
	DateTimeInputFormat types.String `tfsdk:"date_time_input_format"`
	GroupName           types.String `tfsdk:"group_name"`
	HandleErrorMode     types.String `tfsdk:"handle_error_mode"`
	MaxBlockSize        types.Int64  `tfsdk:"max_block_size"`
	MaxRowsPerMessage   types.Int64  `tfsdk:"max_rows_per_message"`
	Name                types.String `tfsdk:"name"`
	NumConsumers        types.Int64  `tfsdk:"num_consumers"`
	PollMaxBatchSize    types.Int64  `tfsdk:"poll_max_batch_size"`
	SkipBrokenMessages  types.Int64  `tfsdk:"skip_broken_messages"`
	Topics              types.Set    `tfsdk:"topics"`
}

// dtoTables request/response object
type dtoTables struct {
	AutoOffsetReset     *string       `groups:"create,update" json:"auto_offset_reset,omitempty"`
	Columns             []*dtoColumns `groups:"create,update" json:"columns"`
	DataFormat          string        `groups:"create,update" json:"data_format"`
	DateTimeInputFormat *string       `groups:"create,update" json:"date_time_input_format,omitempty"`
	GroupName           string        `groups:"create,update" json:"group_name"`
	HandleErrorMode     *string       `groups:"create,update" json:"handle_error_mode,omitempty"`
	MaxBlockSize        *int64        `groups:"create,update" json:"max_block_size,omitempty"`
	MaxRowsPerMessage   *int64        `groups:"create,update" json:"max_rows_per_message,omitempty"`
	Name                string        `groups:"create,update" json:"name"`
	NumConsumers        *int64        `groups:"create,update" json:"num_consumers,omitempty"`
	PollMaxBatchSize    *int64        `groups:"create,update" json:"poll_max_batch_size,omitempty"`
	SkipBrokenMessages  *int64        `groups:"create,update" json:"skip_broken_messages,omitempty"`
	Topics              []*dtoTopics  `groups:"create,update" json:"topics"`
}

// expandTables expands tf object into dto object
func expandTables(ctx context.Context, diags *diag.Diagnostics, o *tfoTables) *dtoTables {
	columnsVar := schemautil.ExpandSetNested[tfoColumns, dtoColumns](ctx, diags, expandColumns, o.Columns)
	if diags.HasError() {
		return nil
	}
	topicsVar := schemautil.ExpandSetNested[tfoTopics, dtoTopics](ctx, diags, expandTopics, o.Topics)
	if diags.HasError() {
		return nil
	}
	return &dtoTables{
		AutoOffsetReset:     schemautil.ValueStringPointer(o.AutoOffsetReset),
		Columns:             columnsVar,
		DataFormat:          o.DataFormat.ValueString(),
		DateTimeInputFormat: schemautil.ValueStringPointer(o.DateTimeInputFormat),
		GroupName:           o.GroupName.ValueString(),
		HandleErrorMode:     schemautil.ValueStringPointer(o.HandleErrorMode),
		MaxBlockSize:        schemautil.ValueInt64Pointer(o.MaxBlockSize),
		MaxRowsPerMessage:   schemautil.ValueInt64Pointer(o.MaxRowsPerMessage),
		Name:                o.Name.ValueString(),
		NumConsumers:        schemautil.ValueInt64Pointer(o.NumConsumers),
		PollMaxBatchSize:    schemautil.ValueInt64Pointer(o.PollMaxBatchSize),
		SkipBrokenMessages:  schemautil.ValueInt64Pointer(o.SkipBrokenMessages),
		Topics:              topicsVar,
	}
}

// flattenTables flattens dto object into tf object
func flattenTables(ctx context.Context, diags *diag.Diagnostics, o *dtoTables) *tfoTables {
	columnsVar := schemautil.FlattenSetNested[dtoColumns, tfoColumns](ctx, diags, flattenColumns, columnsAttrs, o.Columns)
	if diags.HasError() {
		return nil
	}
	topicsVar := schemautil.FlattenSetNested[dtoTopics, tfoTopics](ctx, diags, flattenTopics, topicsAttrs, o.Topics)
	if diags.HasError() {
		return nil
	}
	return &tfoTables{
		AutoOffsetReset:     types.StringPointerValue(o.AutoOffsetReset),
		Columns:             columnsVar,
		DataFormat:          types.StringValue(o.DataFormat),
		DateTimeInputFormat: types.StringPointerValue(o.DateTimeInputFormat),
		GroupName:           types.StringValue(o.GroupName),
		HandleErrorMode:     types.StringPointerValue(o.HandleErrorMode),
		MaxBlockSize:        types.Int64PointerValue(o.MaxBlockSize),
		MaxRowsPerMessage:   types.Int64PointerValue(o.MaxRowsPerMessage),
		Name:                types.StringValue(o.Name),
		NumConsumers:        types.Int64PointerValue(o.NumConsumers),
		PollMaxBatchSize:    types.Int64PointerValue(o.PollMaxBatchSize),
		SkipBrokenMessages:  types.Int64PointerValue(o.SkipBrokenMessages),
		Topics:              topicsVar,
	}
}

var tablesAttrs = map[string]attr.Type{
	"auto_offset_reset":      types.StringType,
	"columns":                types.SetType{ElemType: types.ObjectType{AttrTypes: columnsAttrs}},
	"data_format":            types.StringType,
	"date_time_input_format": types.StringType,
	"group_name":             types.StringType,
	"handle_error_mode":      types.StringType,
	"max_block_size":         types.Int64Type,
	"max_rows_per_message":   types.Int64Type,
	"name":                   types.StringType,
	"num_consumers":          types.Int64Type,
	"poll_max_batch_size":    types.Int64Type,
	"skip_broken_messages":   types.Int64Type,
	"topics":                 types.SetType{ElemType: types.ObjectType{AttrTypes: topicsAttrs}},
}

// tfoColumns Table column
type tfoColumns struct {
	Name types.String `tfsdk:"name"`
	Type types.String `tfsdk:"type"`
}

// dtoColumns request/response object
type dtoColumns struct {
	Name string `groups:"create,update" json:"name"`
	Type string `groups:"create,update" json:"type"`
}

// expandColumns expands tf object into dto object
func expandColumns(ctx context.Context, diags *diag.Diagnostics, o *tfoColumns) *dtoColumns {
	return &dtoColumns{
		Name: o.Name.ValueString(),
		Type: o.Type.ValueString(),
	}
}

// flattenColumns flattens dto object into tf object
func flattenColumns(ctx context.Context, diags *diag.Diagnostics, o *dtoColumns) *tfoColumns {
	return &tfoColumns{
		Name: types.StringValue(o.Name),
		Type: types.StringValue(o.Type),
	}
}

var columnsAttrs = map[string]attr.Type{
	"name": types.StringType,
	"type": types.StringType,
}

// tfoTopics Kafka topic
type tfoTopics struct {
	Name types.String `tfsdk:"name"`
}

// dtoTopics request/response object
type dtoTopics struct {
	Name string `groups:"create,update" json:"name"`
}

// expandTopics expands tf object into dto object
func expandTopics(ctx context.Context, diags *diag.Diagnostics, o *tfoTopics) *dtoTopics {
	return &dtoTopics{Name: o.Name.ValueString()}
}

// flattenTopics flattens dto object into tf object
func flattenTopics(ctx context.Context, diags *diag.Diagnostics, o *dtoTopics) *tfoTopics {
	return &tfoTopics{Name: types.StringValue(o.Name)}
}

var topicsAttrs = map[string]attr.Type{"name": types.StringType}

// Expand public function that converts tf object into dto
func Expand(ctx context.Context, diags *diag.Diagnostics, set types.Set) *dtoUserConfig {
	return schemautil.ExpandSetBlockNested[tfoUserConfig, dtoUserConfig](ctx, diags, expandUserConfig, set)
}

// Flatten public function that converts dto into tf object
func Flatten(ctx context.Context, diags *diag.Diagnostics, m map[string]any) types.Set {
	o := new(dtoUserConfig)
	err := schemautil.MapToDTO(m, o)
	if err != nil {
		diags.AddError("failed to marshal map user config to dto", err.Error())
		return types.SetNull(types.ObjectType{AttrTypes: userConfigAttrs})
	}
	return schemautil.FlattenSetBlockNested[dtoUserConfig, tfoUserConfig](ctx, diags, flattenUserConfig, userConfigAttrs, o)
}
